{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8789bf4c",
   "metadata": {},
   "source": [
    "# Scraping Code for AFE\n",
    "The code below scrapes both the places and finds from the AFE website. afe_places require Selenium to work, is simpler, but is finicky in terms of start up. The finds reads the bare HTML, but due to variation in HTML templates for different kinds of finds, it involves a multipart conditional and is consequently rather long.\n",
    "\n",
    "Strictly speaking, scraping would not be at all needed, since the DAI-AFE website offers a CSV export of its data that exports everything in one go, provided you search without any parameters. The problem is that, while this export provides the find numbers, it does not provide place or hoard numbers, and so prevents the user from re-assembling the data. The virtue of the scraping procedure below is that it also gets those particular numbers.\n",
    "\n",
    "And THIS might not have been a problem either, since one could join on distinct strings and get to the numbers the other way around. But there are doubles in the afe_places df, such that a coin from that place would not know which entry to choose based on the provided value. These all have two entries in the places df: \n",
    "\n",
    "Gudensberg  \n",
    "Karlstadt   \n",
    "Vockerode   \n",
    "Berghofen   \n",
    "Dissen      \n",
    "Hamm        \n",
    "Haffen      \n",
    "Poppenhausen\n",
    "Stockum     \n",
    "Limburg       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are here in case you are debugging the scrapers. They are re-imported in the data cleaning section.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295ad06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The scrapers\n",
    "The import libraries below are common more or less for all the scrapers. Two of three use Selenium. Detailed finds doesn't require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc54e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import io\n",
    "import ast\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen, Request\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def setting_up(sales_url):\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "  req = Request(url=sales_url, headers=headers) \n",
    "  html = urlopen(req).read().decode('utf-8')\n",
    "  soup = BeautifulSoup(urlopen(req).read())\n",
    "  return soup\n",
    "\n",
    "#!pip install selenium\n",
    "#!apt-get update # to update ubuntu to correctly run apt install\n",
    "#!apt install chromium-chromedriver\n",
    "#!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "\n",
    "#!pip install webdriver_manager\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#sys.path.insert(0,'/opt/homebrew/Caskroom/chromedriver')\n",
    "#from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e0ce",
   "metadata": {},
   "source": [
    "### Place Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f930db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code works to grab what is needed from the places website using Selenium\n",
    "# In addition to Lee's code, I have added a tester that continues the loop\n",
    "# in cases of failure. \n",
    "# After every loop, I am also outputing a new copy of the csv, which will help with\n",
    "# debug in cases where this fails, say, 1500 rows in.\n",
    "\n",
    "for i in range(1, 10):\n",
    "    link = 'http://afe.dainst.org/place?afeid=' + str(i)\n",
    "\n",
    "    try:\n",
    "        driver.maximize_window()\n",
    "        driver.implicitly_wait(1000)\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(1000)\n",
    "        driver.find_element(By.CLASS_NAME,'lclinks')\n",
    "\n",
    "        html = driver.page_source\n",
    "        #print(pd.read_html(html)[0])\n",
    "        places_temp = pd.read_html(html)[0].set_index('Name')\n",
    "        #print(places_temp.loc['ID'][0])\n",
    "\n",
    "        new_line = [places_temp.loc['ID'][0], places_temp.loc['Name'][0],places_temp.loc['Längen- und Breitengrad'][0].split(',')[0],places_temp.loc['Längen- und Breitengrad'][0].split(',')[1]]\n",
    "        places.loc[i] = new_line\n",
    "        places.to_csv('afe_places.csv', index=False)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9470540",
   "metadata": {},
   "source": [
    "### \"Detailed Result\" Scraper\n",
    "This scrapes what is essentially the report of individual coins. This is where the majority of our information comes from. It is a difficult scrape. We aren't using Selenium, but instead just targeting text patterns and grabbing whatever comes up in them. This results in some incredibly messy columns, occasionally grabbing the entire HTML text and sticking it into individual columns. I have, through Regex, tried to tame these. The occasional outlier gets through, even by the end, which you will see me weed out of the results in mint conversion stage of data cleaning (not even in this section... way down the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ff15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works, and covers a number of different variations in the HTML template used to generate pages\n",
    "# So far used on #1-5000\n",
    "\n",
    "start_id = 15001\n",
    "end_id = 18000\n",
    "\n",
    "cols = ['ID', 'place_id', 'afe_find_id', 'Location', 'Status', 'Denomination', 'Issuer', 'Mint', 'Date', 'References', 'Remarks', 'Bibliography']\n",
    "AFE = pd.DataFrame(columns=cols)\n",
    "\n",
    "while start_id <= end_id:\n",
    "    url = 'http://afe.dainst.org/detailedresult?l=en&link=' + str(start_id)\n",
    "    links = setting_up(url).find_all('a')\n",
    "    place_link = str(links[0])\n",
    "    blank = '<a href=\"http://afe.dainst.org/coin?afeid=&amp;l=en\" target=\"_blank\">http://afe.dainst.org/coin?afeid=</a>'\n",
    "    \n",
    "    # A big problem in the AFE implementation is that links come in various orders.\n",
    "    # We had solved this by accounting for fixed places in which links came\n",
    "    # But these turned out not to be so fixed. So I've put in a simple \n",
    "    # loop that determines the right link for each entry. This should fix \n",
    "    # \"invalid literal\" errors that pop up when you pass the wrong formatted link\n",
    "    # down the pipeline\n",
    "\n",
    "    \n",
    "    image = '.jpg'\n",
    "    if place_link == blank:\n",
    "        start_id += 1\n",
    "        print(start_id)\n",
    "        time.sleep(0.2)\n",
    "        continue\n",
    "    elif image in place_link:\n",
    "        # The link number differs for image pages than not (say AFE 4 or 5). This is one difference.\n",
    "        #place_link = str(links[2])\n",
    "        #print(place_link)\n",
    "        \n",
    "        marker = \"place?afeid=\"\n",
    "        for i in links:\n",
    "            if marker in str(i):\n",
    "                place_link = str(i)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        temp = setting_up(url).get_text()\n",
    "        #print(temp)\n",
    "\n",
    "        loc = temp.find('Location')\n",
    "        sta = temp.find('Status')\n",
    "        deno = temp.find('Denomination')\n",
    "        iss = temp.find('Issuer')\n",
    "        mint = temp.find('Mint')\n",
    "        dat = temp.find('Date')\n",
    "        ref = temp.find('References')\n",
    "        rem = temp.find('Remarks')\n",
    "        bib = temp.find('Bibliography')\n",
    "        end_text = temp.find('Export CSV')-22\n",
    "        #print(loc,sta,deno,iss,mint,dat,ref,rem,bib,end_text)\n",
    "\n",
    "        # One major problem is that various fields wind up being empty in the AFE. When they are\n",
    "        # It breaks the code, since the fields simply disappear. These produce a \"-1\" value when\n",
    "        # Evaluatd by the above \"find\" functions. The code below is a diagnostic to show which fields \n",
    "        # Are empty\n",
    "        #fields = [loc,sta,deno,iss,mint,dat,ref,rem,bib,end_text]\n",
    "\n",
    "        #for n, i in enumerate(fields):\n",
    "        #    if i == -1:\n",
    "        #        fields[n] = 'empty'\n",
    "        #    else:\n",
    "        #        continue\n",
    "\n",
    "\n",
    "        # The problem with image pages is that some have status fields and others do not. It is inconsistent.\n",
    "        # so we need to build in a conditional for this. this is done for each variable below, where it is \n",
    "        # known to be a problem\n",
    "        if sta != -1:\n",
    "            loc_string = temp[loc+8:sta-1]\n",
    "            sta_string = temp[sta+6:deno]\n",
    "        else:\n",
    "            loc_string = temp[loc+8:deno]\n",
    "            sta_string = 'No status'\n",
    "\n",
    "        deno_string = temp[deno+12:iss]\n",
    "\n",
    "        if temp.find('Issued for') > 0: iss_string = temp[iss+6:temp.find('Issued for')]\n",
    "        else: iss_string = temp[iss+6:mint]\n",
    "\n",
    "        min_string = temp[mint+4:dat]\n",
    "\n",
    "        if ref > 0: dat_string = temp[dat+4:ref]\n",
    "        else: dat_string = temp[dat+4:rem]\n",
    "\n",
    "        if (temp.find('Obv./Rev.') > 0) and (ref > 0): ref_string = temp[ref+10:temp.find('Obv./Rev.')]\n",
    "        elif (temp.find('Obv./Rev.') == -1): ref_string = ''\n",
    "        elif (ref > 0): ref_string = temp[ref+10:rem]\n",
    "        else: ref_string = 'No reference'\n",
    "\n",
    "        if bib > 0: \n",
    "            rem_string = temp[rem+7:bib]\n",
    "        elif bib == -1: \n",
    "            rem_string = 'No remarks'\n",
    "\n",
    "        bib_string = temp[bib+12:end_text]\n",
    "        #print(loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string, sep='; ')\n",
    "\n",
    "        #print(setting_up(url).find('#place?afeid='))\n",
    "        #links = setting_up(url).find_all('a')\n",
    "\n",
    "        place_start = place_link.find('place?afeid=') + 12\n",
    "        place_end = place_link.find('\" tar')\n",
    "        #print(place_start, place_end)\n",
    "\n",
    "        place_id = int(place_link[place_start:place_end])\n",
    "\n",
    "        #The same problem above is replicated in the find_spot link position.\n",
    "        #Also created an \"invalid literal\" error\n",
    "        find_marker = \"findspot?afeid=\"\n",
    "        for i in links:\n",
    "            if find_marker in str(i):\n",
    "                find_link = str(i)\n",
    "            else:\n",
    "                continue\n",
    "        #find_link = str(links[2])\n",
    "        link_start = find_link.find('findspot?afeid=') + 15\n",
    "        link_end = find_link.find('\" tar')\n",
    "        link_id = int(find_link[link_start:link_end])\n",
    "\n",
    "\n",
    "        #print(place_id, link_id, sep='; ')\n",
    "        new_line = [start_id, place_id, link_id, loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string]\n",
    "        AFE.loc[start_id] = new_line\n",
    "\n",
    "        start_id += 1\n",
    "        print(start_id)\n",
    "        time.sleep(0.2)\n",
    "        continue\n",
    "    else:\n",
    "        #url = 'http://afe.dainst.org/detailedresult?l=en&link=' + str(start_id)\n",
    "        temp = setting_up(url).get_text()\n",
    "        loc = temp.find('Location')\n",
    "        sta = temp.find('Status')\n",
    "        deno = temp.find('Denomination')\n",
    "        iss = temp.find('Issuer')\n",
    "        mint = temp.find('Mint')\n",
    "        dat = temp.find('Date')\n",
    "        ref = temp.find('References')\n",
    "        rem = temp.find('Remarks')\n",
    "        bib = temp.find('Bibliography')\n",
    "        end_text = temp.find('Export CSV')-22\n",
    "\n",
    "        loc_string = temp[loc+8:sta-1]\n",
    "        sta_string = temp[sta+6:deno]\n",
    "        deno_string = temp[deno+12:iss]\n",
    "        if temp.find('Issued for') > 0: iss_string = temp[iss+6:temp.find('Issued for')]\n",
    "        else: iss_string = temp[iss+6:mint]\n",
    "        min_string = temp[mint+4:dat]\n",
    "        if ref > 0: dat_string = temp[dat+4:ref]\n",
    "        else: dat_string = temp[dat+4:rem]\n",
    "        if (temp.find('Obv./Rev.') > 0) and (ref > 0): ref_string = temp[ref+10:temp.find('Obv./Rev.')]\n",
    "        elif (ref > 0): ref_string = temp[ref+10:rem]\n",
    "        else: ref_string = ''\n",
    "        rem_string = temp[rem+7:bib]\n",
    "        bib_string = temp[bib+12:end_text]\n",
    "        #print(loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string, sep='; ')\n",
    "\n",
    "\n",
    "        #print(setting_up(url).find('#place?afeid='))\n",
    "        links = setting_up(url).find_all('a')\n",
    "        #place_link = str(links[0])\n",
    "        place_start = place_link.find('place?afeid=') + 12\n",
    "        place_end = place_link.find('\" tar')\n",
    "        place_id = int(place_link[place_start:place_end])\n",
    "\n",
    "        find_link = str(links[1])\n",
    "        link_start = find_link.find('findspot?afeid=') + 15\n",
    "        link_end = find_link.find('\" tar')\n",
    "        link_id = int(find_link[link_start:link_end])\n",
    "\n",
    "\n",
    "        #print(place_id, link_id, sep='; ')\n",
    "        new_line = [start_id, place_id, link_id, loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string]\n",
    "        AFE.loc[start_id] = new_line\n",
    "\n",
    "        start_id += 1\n",
    "        time.sleep(0.2)\n",
    "        print(start_id)\n",
    "AFE.to_csv('afe_detailed_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e673d92",
   "metadata": {},
   "source": [
    "### Find Scraper (Fund Ort)\n",
    "This one was pretty simple. Another instance of Selenium scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5115778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the previous scraping is not sufficient because we are missing find categories (hoards, etc.)\n",
    "# We need to scrape the fundort pages too\n",
    "# This requires Selenium :-(\n",
    "\n",
    "# This code works to grab what is needed from the places website using Selenium\n",
    "# In addition to Lee's code, I have added a tester that continues the loop\n",
    "# in cases of failure. \n",
    "# After every loop, I am also outputing a new copy of the csv, which will help with\n",
    "# debug in cases where this fails, say, 1500 rows in.\n",
    "#!pip install webdriver_manager\n",
    "\n",
    "# We ran into some problems, and it had to do with the reset timing on the Selenium browser. \n",
    "# I built a test function that basically told the program to continue a loop if it did not \n",
    "# resolve within 5 seconds (thus getting past non-conforming URLs). \n",
    "# But the browser had to be reset for the next step too---except the browser auto-reset \n",
    "# was 1000 seconds and nothing about our reset test triggered this. \n",
    "# Thus all future loops resolved without any input from the browsers (they were all failed loops). \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# There are a lot of empty spaces in the finds database.\n",
    "# They take a long time to resolve, so any find that takes longer than\n",
    "# A certain amount of time is an error. We build in a timer to resolve\n",
    "# These errors\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "# You have to create the DF in advance, because if the try function fails, subsequent\n",
    "# attempts, including in except will have no df to latch onto.\n",
    "# Lee did not need this in the places scrape, since there is a successful place at \n",
    "# loc #1. There is not one here.\n",
    "data = [['','','','']]\n",
    "finds = pd.DataFrame(data, columns=['ID', 'Name', 'Fundkategorie', 'Link'])        \n",
    "\n",
    "for i in range(1,4018):\n",
    "    link = 'http://afe.dainst.org/findspot?afeid=' + str(i)\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(4)\n",
    "    driver.get(link)\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    \n",
    "    #print(pd.read_html(html)[0])\n",
    "    try:\n",
    "        with time_limit(5):\n",
    "            driver.find_element(By.CLASS_NAME,'lclinks')\n",
    "\n",
    "            html = driver.page_source\n",
    "\n",
    "            finds_temp = pd.read_html(html)[0].set_index('Name')\n",
    "            #print(places_temp.loc['ID'][0])\n",
    "\n",
    "            new_line = [finds_temp.loc['ID'][0], finds_temp.loc['Name'][0],finds_temp.loc['Fundkategorie'][0],finds_temp.loc['Link'][0]]\n",
    "            finds.loc[i] = new_line\n",
    "            print(i, \"Success!\")\n",
    "            finds.to_csv('afe_fundort.csv', index=False)\n",
    "    except:\n",
    "        new_line = ['', '','','']\n",
    "        finds.loc[i] = new_line\n",
    "        print(i, \"Empty\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd505ba",
   "metadata": {},
   "source": [
    "# Converting Scraped Data to Our Format\n",
    "We start with the Detailed Finds report and work off of that, as that is the main source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 5000) \n",
    "pd.set_option('display.max_rows', 5000)\n",
    "dai = pd.read_csv('afe_detailed_results.csv')\n",
    "\n",
    "# Some elements messing up the map\n",
    "dai['Location'] = dai['Location'].str.replace('(','')\n",
    "dai['Location'] = dai['Location'].str.replace(')','')\n",
    "dai['Location'] = dai['Location'].str.replace('?','')\n",
    "dai['Location'] = dai['Location'].str.replace('\"','')\n",
    "\n",
    "# Replace NaNs in the YearFound column with 1700, since blank years prevents those\n",
    "# rows from being summed along with the other ones\n",
    "\n",
    "#dai['YearFound'] = dai['YearFound'].fillna(1700)\n",
    "dai['Status'] = dai['Status'].fillna('Einzelfund_FC')\n",
    "\n",
    "# There are many values in dai['Date'] that simply won't yield a date range. So we\n",
    "# Are doing some Regex work here.\n",
    "\n",
    "# This is a an error that occurs when nothing is captured for the Date value\n",
    "# It throws in the entire HTML output of the scraper -- not useful\n",
    "# We replace that with \"malformed\"\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"[.|\\n|\\W|\\w]*Detailed Result[.|\\n|\\W|\\w]*\", \"Malformed\", i)\n",
    "\n",
    "# This replaces all '\\n' entries in bad values that do not conform to the above\n",
    "# The '\\n' interferes with identifying date ranges, setting up a succesful\n",
    "# Targeting operation in the next loop\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(.*)(\\\\n)(.*)\", \"\\\\1\\\\3\", i)\n",
    "\n",
    "# Need to replace 'bis' with '-'\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"bis\", \"-\", i)\n",
    "\n",
    "# What follows is a pretty straightforward series of transformations of bad formats\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(LT)(.*)\", \"Unclear Periodization\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(nach\\s)(\\d+)\", \"\\\\2-750\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(\\()(\\d+-\\d+)\", \"\\\\2\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"Zeitstellung unbekannt*\", \"Date Unknown\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c42716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop deals exclusively with transforming century ranges into hard date ranges\n",
    "# It relies exlusively on re\n",
    "\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    # there are a few common kinds of tags for dross. Everything after these keywords is garbage\n",
    "    entry = re.sub(\"(.*)(Bibliography)(.*)\", \"\\\\1\", i)\n",
    "    entry = re.sub(\"(.*)(Secondary)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Weight)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Obv)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Peculiarities)(.*)\", \"\\\\1\", entry)\n",
    "    # After this, we have some conditionals. These account for differences in form btw\n",
    "    # Single century dates, as well as BCE dates (those with only BCE are cut entirely, and\n",
    "    # those that straddle BCE/CE are assigned a 1-??? date range)\n",
    "\n",
    "    if re.search(\"Jh.|Jahrhundert|\\d+\\.\", i) != None:\n",
    "        try:\n",
    "            # The len(???) conditional assesses how many numbers are present in a field.\n",
    "            # If 2, then it is a range, if 1 then not\n",
    "            if len(re. findall(\"\\d+\", entry)) == 1 and re.search(\"v\\.\\sChr\\.\", i) != None:\n",
    "                century_start = '0'\n",
    "                century_end = '0'\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 1:\n",
    "                century = re.sub(\"(.*)(\\d+)(.*)\", \"\\\\2\", entry)\n",
    "                century_start = int(century) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"Jahrhundert|Jh\", i) == None:\n",
    "                century_start = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", entry)\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", entry)\n",
    "                century_start = int(century_start) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"v\\.\\sChr\\.\", i) == None:\n",
    "                century_start = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", entry)\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", entry)\n",
    "                century_start = int(century_start) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"v\\.\\sChr\\.\", i) != None and re.search(\"n\\.\\sChr\\.\", i) != None:\n",
    "                century_start = '1'\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            else:\n",
    "                century_start = \"0\"\n",
    "                century_end = \"0\"\n",
    "\n",
    "            if century_end == '099':\n",
    "                century_end = '99'\n",
    "\n",
    "            date = century_start + \"-\" + century_end\n",
    "            dai['Date'].loc[row] = date\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd97bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is looking for any lone century dates (e.g., 1. Jh.) by looking for 'Jh.' within a certain number\n",
    "# of characters of 1 (or whatever digit it may be). It seems to work pretty well.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if re.search(\"(\\d+)(\\.)(?![.!?] )\\W+(?:\\w+(?![.!?] )\\W+){1,2}?(Jh.){1,2}?(?![.!?] )\\W+(?:\\w+(?![.!?] )\\W+)\", i) != None:\n",
    "        print(row, i)\n",
    "        century = re.sub(\"(\\d+)(\\.)(.*)\", \"\\\\1\", i)\n",
    "        century = int(century) - 1\n",
    "        century = str(century) + '00'\n",
    "        if century == '000':\n",
    "            century = '1'\n",
    "        print(century)\n",
    "\n",
    "# Final BCE/CE cleanup, now not just on centuries\n",
    "# This loop gets rid of anything remaining that straddles BCE/CE since this is causing problems\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 2 and re.search(\"v\\.\", i) != None and re.search(\"n\\.\", i) != None:\n",
    "        century_start = '1'\n",
    "        century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "        date = century_start + \"-\" + century_end\n",
    "        dai['Date'].loc[row] = date\n",
    "\n",
    "# And single dates in BCE are eliminated\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 1 and re.search(\"v\\.\", i) != None:\n",
    "        dai['Date'].loc[row] = '0'\n",
    "\n",
    "# All date ranges that are BCE only\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 2 and re.search(\"v\\.\", i) != None and re.search(\"n\\.\", i) == None:\n",
    "        dai['Date'].loc[row] = '0'\n",
    "\n",
    "# This one, having solved the '\\n's actually replaces everything but the date range\n",
    "# It works surprisingly well. Previously I ran this at the start of the filtering\n",
    "# sequence, but it turns out to work much better if it comes at the very end.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(\\d+-\\d+|\\d+)(.*)\", \"\\\\1\", i)\n",
    "\n",
    "# The one category of string artifact that seems to get through the previous round of re filters \n",
    "# and messes with the next bit of code in the process, is '.ca' (and 'Ende', as it turns out), \n",
    "# so these are removed at this stage\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(.*)ca.\\s(.*)|(.*)(Ende\\s)(.*)\", \"\\\\2\", i)\n",
    "\n",
    "# This loop deals with an artifact created by previous regex steps. Getting the regex order is finnicky so it applies\n",
    "# to 99% of cases. It produces these strings where 00 and 99 have been added to normal date ranges\n",
    "# It thinks they are centuries. Anyway, the easiest solution is to ID these and to correct them by \n",
    "# subtracting 2 characters from the end of the string. This does that.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re. findall(\"\\d+\", i)) == 2:\n",
    "        first = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", i)\n",
    "        second = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "        if len(first) > 3:\n",
    "            first = first[:-2]\n",
    "            # A previous step seems to -1 these (but not other ranges), so I correct here\n",
    "            first = int(first) + 1\n",
    "            first = str(first)\n",
    "        if len(second) > 3:\n",
    "            second = second[:-2]\n",
    "            second = int(second) + 1\n",
    "            second = str(second)\n",
    "        newrange = first+'-'+second\n",
    "        dai['Date'].loc[row] = newrange\n",
    "\n",
    "# Create two columns from the Date column\n",
    "dai['Date Min'] = ''\n",
    "dai['Date Max'] = ''\n",
    "\n",
    "# This separates dateranges that remain into useful elements for date min and max\n",
    "# the len business below is simply to see how many dates we are working with\n",
    "# So len 1 means there is just a single year, len 0 means there is not a meaningful year\n",
    "# and len 2 is a standard date range\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re. findall(\"\\d+\", i)) == 0:\n",
    "        dai['Date Min'].loc[row] = ''\n",
    "        dai['Date Max'].loc[row] = ''\n",
    "    if len(re. findall(\"\\d+\", i)) == 1:\n",
    "        dai['Date Min'].loc[row] = i\n",
    "        dai['Date Max'].loc[row] = i\n",
    "    if len(re. findall(\"\\d+\", i)) == 2:\n",
    "        dai['Date Min'].loc[row] = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", i)\n",
    "        dai['Date Max'].loc[row] = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "\n",
    "# We have some detritus that will prevent a successful conversion of the column to int\n",
    "# This removes that\n",
    "for row, i in enumerate(dai['Date Min']):\n",
    "    if re.findall(\"(vor\\s|\\(|-)(\\d+)\", i) != None:\n",
    "        dai['Date Min'].loc[row] = re.sub(\"(vor\\s|\\(|-)(\\d+)\", \"\\\\2\", i)\n",
    "\n",
    "for row, i in enumerate(dai['Date Max']):\n",
    "    if re.findall(\"(vor\\s|\\(|-)(\\d+)\", i) != None:\n",
    "        dai['Date Max'].loc[row] = re.sub(\"(vor\\s|\\(|-)(\\d+)\", \"\\\\2\", i)\n",
    "\n",
    "# The dates in this row somehow survived everything and generated a bad MinMax Date range\n",
    "# Just drop it here\n",
    "dai.drop(index=dai[dai['ID'] == 16989].index, inplace=True)\n",
    "dai.drop(index=dai[dai['Date Min'] == ''].index, inplace=True)\n",
    "dai.drop(index=dai[dai['Date Min'] == '0'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotta reset index to facilitate looping\n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0787c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of strings in these columns to integers\n",
    "dai = dai.astype({\"Date Min\": int, \"Date Max\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bf7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some final weeding out of BCE related entries\n",
    "# First by getting rid of anything where DateMin is greater than DateMax\n",
    "for row, i in enumerate(dai['Date Min']):\n",
    "    if i > dai['Date Max'].loc[row]:\n",
    "        dai.drop([row], inplace=True)\n",
    "        \n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then removing anything with Republik under ruler\n",
    "dai = dai[dai['Issuer'] != 'Republik']\n",
    "dai = dai[dai['Date Min'] > 324]\n",
    "dai = dai[dai['Date Max'] < 751]\n",
    "\n",
    "\n",
    "# There is a strange phenomenon where 4 entries of Roman emperors have date ranges transposed,\n",
    "# appearing to be much later than they are (e.g., 600-699). I simply remove them here\n",
    "# esp since none would fall into our date range\n",
    "dai = dai[dai['Issuer'] != 'Antoninus Pius']\n",
    "dai = dai[dai['Issuer'] != 'Domitianus']\n",
    "dai = dai[dai['Issuer'] != 'Commodus']\n",
    "dai = dai[dai['Issuer'] != 'Nerva']\n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d038ed",
   "metadata": {},
   "source": [
    "## Linking to other datasets and producing group/find dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a814ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we will link Place to Find information, specifically for geogrpahic coords\n",
    "places = pd.read_csv('afe_places.csv')\n",
    "places = places.rename(columns={\"ID\": \"place_id\"})\n",
    "#places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ada671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dai (output of last section) with places. produces daip (though I suppose I could keep \n",
    "# the name as dai---oh well.)\n",
    "daip = pd.merge(dai, places,  how='left', left_on=['place_id'], right_on = ['place_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import the Fund Ort df in order to get type_find\n",
    "# then we merge with existing daip\n",
    "fundort = pd.read_csv('afe_fundort.csv')\n",
    "daip = pd.merge(daip, fundort,  how='left', left_on=['afe_find_id'], right_on = ['ID'])\n",
    "daip = daip.drop(['ID_y', 'Name', 'Link'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647593a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin from detailed finds df, and implicitly at this level of ontology\n",
    "# we are dealing with single coins. We will be summing these to produce finds\n",
    "# at that point the qunatities will turn into aggregates\n",
    "# So this creates those quantities\n",
    "daip['Quantity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the goal is to create groups by summing the Quantity column along multiple indexes\n",
    "group = daip.groupby(['Location','afe_find_id','Mint','Denomination','Date Min','Date Max','Issuer'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "# The process yields a Pandas series. That series should be turned into a df\n",
    "group = group.to_frame()\n",
    "# That df needs to level out the indexes back into normal values.\n",
    "# Each index value simply becomes a repeated value in the relevant cells\n",
    "group = group.reset_index(level=['Location','afe_find_id','Date Min', 'Date Max', 'Denomination','Mint','Issuer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create distinct group numbers. This covnerts the df index into that.\n",
    "group = group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25393926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This turns those group numbers into display labels for the map\n",
    "group['coin_group_id'] = ''\n",
    "for row, i in enumerate(group.index):\n",
    "    group['coin_group_id'].loc[row] = 'AFE-' + str(i +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219749e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to rename a bunch of these columns\n",
    "group = group.rename(columns={\"index\": \"group_id\", \"\" \"Location\": \"name\", \"Issuer\" : \"ruler\", \"Denomination\" : \"denomination\",\\\n",
    "                     \"Quantity\" : \"num_coins\", \"Mint\" : \"mint\", \"Date Min\" : \"start_year\", \"Date Max\" : \"end_year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there are some columns where values need to be added. I am keeping the excavation year column blank for now\n",
    "today = datetime.date.today()\n",
    "group['created'] = today.strftime(\"%m-%d-%Y\")\n",
    "group['imported'] = today.strftime(\"%m-%d-%Y\")\n",
    "group['owner'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert denominations to our format before we assign metals\n",
    "# This was taken almost wholesale from Lee's CHRE code\n",
    "\n",
    "# For future reference, to find all denoms\n",
    "# group.denomination.unique()\n",
    "\n",
    "denomination_conversion = {\n",
    "    'Denarius':'denarius', 'Semis':'semissis',\n",
    "       'Aureus':'aureus', 'AE':'uncertain (bronze)', 'Antoninianus':'radiate or nummus (UK find)',\n",
    "       'Follis':'follis','AE2':'AE2 (5.15g)', 'Drachme':'drachm', 'AV':'AV', 'Maiorina':'follis',\n",
    "       'AE3':'AE3 (2.58g)', 'Solidus':'solidus', 'Silber':'uncertain (silver)', 'Siliqua':'siliqua', \n",
    "    'AE4':'AE4 (1.23g)', 'Tremissis':'tremissis', '10 Num' : '10 nummi', '2 Solidi' : '2 solidi',\n",
    "    'Siliqua (reduziert)' : 'reduced siliqua', 'Miliarensis' : 'miliarensis'\n",
    "}\n",
    "\n",
    "obsolete_denominations = ['Sestertius', 'As', 'Tetradrachme', 'Centenionalis', 'Dupondius',  'Quadrans', 'Doppelsestertius', 'Dupondius / As']\n",
    "\n",
    "# actual conversion of denominations to FLAME style\n",
    "group = group[~group['denomination'].isin(obsolete_denominations)]\n",
    "new_list = group['denomination'].fillna('Uncertain').apply(lambda x:denomination_conversion[x])\n",
    "group['denomination'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225acf4f-c8d9-4c67-b5b8-c8767aaec2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follis conversion. After the above conversion, the dataset produced got some complaints specifically\n",
    "# in the realm of the follis. Dictionary won't do here, we need a function.\n",
    "\n",
    "def follinator(yearmin, yearmax):\n",
    "    if yearmin >= 325 and yearmax <= 336:\n",
    "        denomination = 'aes/follis'\n",
    "    elif yearmin >= 337 and yearmax <= 347:\n",
    "        denomination = 'AE3'\n",
    "    elif yearmin >= 348 and yearmax <= 354:\n",
    "        denomination = 'uncertain (bronze)'\n",
    "    elif yearmin >= 355 and yearmax <= 360:\n",
    "        denomination = 'AE3'\n",
    "    elif yearmin >= 361 and yearmax <= 497:\n",
    "        denomination = 'uncertain (bronze)'\n",
    "    elif yearmin >= 498 and yearmax <= 750:\n",
    "        denomination = '40 nummi (follis)'\n",
    "    else:\n",
    "        denomination = 'uncertain (bronze)'\n",
    "    return denomination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d51983-9f67-47a8-93d1-5a97acf90c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This applies follinator, bringing in denominations as a third column to iterate on\n",
    "group['denomination'] = group[['denomination','start_year','end_year']].apply(lambda x: follinator(x['start_year'],x['end_year']) if x['denomination'] == 'follis' else x['denomination'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032b2dd-136c-46de-a064-87bfcf696c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ea8ee-b3c1-4f87-9b6f-806da9a81b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert denominations to metal\n",
    "# Generate metal\n",
    "metal_conversion = {\n",
    "   'denarius':'silver',\n",
    "    'semissis':'gold',\n",
    "    'aureus':'gold',\n",
    "    'uncertain (bronze)':'bronze',\n",
    "    'radiate or nummus (UK find)':'bronze',\n",
    "    'follis':'bronze', \n",
    "    'AE2 (5.15g)':'bronze',\n",
    "    'drachm':'silver',\n",
    "    'AV':'gold',\n",
    "    'AE3 (2.58g)':'bronze',\n",
    "    'solidus':'gold',\n",
    "    'uncertain (silver)':'silver',\n",
    "    'siliqua':'silver',\n",
    "    'AE4 (1.23g)':'bronze',\n",
    "    'tremissis':'silver',\n",
    "    '10 nummi':'bronze',\n",
    "    '2 solidi':'gold',\n",
    "    'reduced siliqua':'silver',\n",
    "    'miliarensis':'silver'\n",
    "}\n",
    "\n",
    "other_metals = []\n",
    "group['metal'] = \"\"\n",
    "group = group[~group['metal'].isin(other_metals)]\n",
    "new_list = group['denomination'].fillna('Uncertain').apply(lambda x:metal_conversion[x])\n",
    "group['metal'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1999278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to convert mints.\n",
    "#group.mint.unique()\n",
    "mint_conversion = {\n",
    "    'Roma':'Roma', 'Alexandria':'Alexandria ad Aegyptum', 'Uncertain mint':'Unknown', 'Constantinopolis':'Constantinople',\n",
    "       'Unofficial mint':'Unknown', 'Siscia':'Siscia', 'Thessalonica':'Thessalonika', 'Londinium':'Londinium',\n",
    "       'Treveri':'Colonia Augusta Treverorum', 'Lugdunum':'Lugdunensium', 'Ticinum':'Ticinum', 'Aquileia':'Aquileia', 'Colonia CAA':'Unknown',\n",
    "       'Antiochia':'Antioch', 'Emerita':'Emerita', 'Lycia':'Unknown (East Roman)', 'Cyzicus':'Kyzikos', \n",
    "    'Roma / Lugdunum':'Roma or Lugdunum', #new mint\n",
    "       'Sirmium':'Sirmium', 'Eastern mint':'Unknown (East Roman)', 'Gallia':'Unknown (Gaul)', 'Unidentified mint':'Unknown',\n",
    "       'Laodicea ad Mare':'Laodicea ad Mare', #new mint\n",
    "    'Mediolanum':'Mediolanum', 'Ravenna':'Ravenna', 'Roma / Tarraco (?)':'Roma or Tarracona', #new mint\n",
    "       'Africa':'Unknown (Africa)', #new mint\n",
    "    'Hispania':'Unknown (Iberia)', 'Colonia Caesaraugusta':'Cesaraugusta', 'Greek East':'Unknown (East Roman)',\n",
    "    'Münzstätte nicht bekannt':'Unknown (Germany)', 'Östliche Münzstätte' : 'Unknown (Germany)',\n",
    "    '\\n\\n\\n\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nID: 11443\\n\\n\\nName\\nValue\\n\\n\\nID11443LocationBad Sulza, Bad SulzaStatusOfficialDenominationFollisIssuerConstans':'Bad Sulza',\n",
    "    '\\n\\n\\n\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nID: 11444\\n\\n\\nName\\nValue\\n\\n\\nID11444LocationBad Sulza, Bad SulzaStatusOfficialDenominationFollisIssuerConstantius II.':'Bad Sulza',\n",
    "    'Irreguläre Münzstätte' : 'Unknown (Germany)', 'Sicilia' : 'Sicily', 'Syrien' : 'Unknown (Greater Syria)',\n",
    "    'Arelate' : 'Arelato', 'Heracleia' : 'Heraclea'\n",
    "}\n",
    "\n",
    "obsolete_mints = ['Nicomedia']\n",
    "\n",
    "group = group[~group['mint'].isin(obsolete_mints)]\n",
    "new_list = group['mint'].fillna('Uncertain').apply(lambda x:mint_conversion[x])\n",
    "group['mint'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ec4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final regex cleanup of some ruler names, which went through the original regex process (see stages above)\n",
    "# in the scraping and cleaning stages\n",
    "# And picked up some odd elements\n",
    "x = lambda a : re.sub('(.*)(Date)(.*)|(\\\\n.*)', '\\\\1', a)\n",
    "group['ruler'] = group['ruler'].map(x)\n",
    "\n",
    "\n",
    "\n",
    "#for i in group['ruler']:\n",
    "#    print(x(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.to_csv(path_or_buf='final_afe_groups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479b679",
   "metadata": {},
   "source": [
    "### Create Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89005be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving values NaN in any row of daip means that when you create a new df, they will\n",
    "# be dropped. You need to provide some value\n",
    "#daip[['Fundkategorie','References']] = daip[['Fundkategorie','References']].fillna('Unknown')\n",
    "daip['Fundkategorie'] = daip['Fundkategorie'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "daip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a final problem. The original scrape and regex cut the last letter off of some finds\n",
    "# This is the result of having to filter through huge variation in string patterns\n",
    "# I don't think an ideal pattern matching formula exists, so rather than play with that\n",
    "# We just reinput them from afe_fundort\n",
    "fundort = pd.read_csv('afe_fundort.csv')\n",
    "# Drop nans\n",
    "fundort = fundort.dropna(subset=['ID'])\n",
    "fundort = fundort.reset_index(drop=True)\n",
    "# Convert to int, not str. Find IDs are ints\n",
    "fundort['ID'] = fundort['ID'].astype(int)\n",
    "def find_corrector(find_id):\n",
    "    find_name = fundort[fundort['ID']==find_id]['Name'].values[0]\n",
    "    return find_name\n",
    "daip['Location'] = daip['afe_find_id'].map(find_corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create finds using the same method.\n",
    "# We can fill in information in a subsequent step, by joining between the two\n",
    "#find = daip.groupby(['afe_find_id','Location','Fundkategorie','lat','long','References'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "find = daip.groupby(['afe_find_id','Location','Fundkategorie','lat','long'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "# The process yields a Pandas series. That series should be turned into a df\n",
    "find = find.to_frame()\n",
    "# That df needs to level out the indexes back into normal values.\n",
    "# Each index value simply becomes a repeated value in the relevant cells\n",
    "#find = find.reset_index(level=['afe_find_id','Location','Fundkategorie','lat','long','References'])\n",
    "find = find.reset_index(level=['afe_find_id','Location','Fundkategorie','lat','long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "find['year_found'] = '1700'\n",
    "find['year_found_end'] = today.strftime(\"%Y\")\n",
    "find['comments'] = ''\n",
    "find['imported'] = today.strftime(\"%m-%d-%Y\")\n",
    "find['references'] = ''\n",
    "for row, i in enumerate(find['afe_find_id']):\n",
    "    find['references'].loc[row] = 'http://afe.dainst.org/findspot?afeid=' + str(find['afe_find_id'].loc[row])\n",
    "find['owner'] = 'DAI'\n",
    "find['created'] = today.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create all of the columns we can.\n",
    "# The only one not possible yet is startDate and endDate, which requires\n",
    "# work to be done on coin groups (return to it below)\n",
    "find['hoard?'] = find['Fundkategorie'] == 'Schatzfund'\n",
    "find['single?'] = find['Fundkategorie'] == 'Einzelfund'\n",
    "find['excavation?'] = find['Fundkategorie'] == 'Kollektivfund'\n",
    "find['excavation?'] = find['Fundkategorie'] == 'Grabfund'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b93a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "find = find.rename(columns={\"afe_find_id\":\"find_number\",\"Location\":\"Name\",\"Fundkategorie\":\"type_find\",\"Quantity\":\"num_coins\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert find types from German to English. Some additional work needed for categories that don't fit\n",
    "# e.g., kollektivfund\n",
    "find_conversion = {\n",
    "    'Einzelfund':'Single Find',\n",
    "    'Schatzfund':'Hoard Find',\n",
    "    'Excavation':'Excavation Find'\n",
    "}\n",
    "\n",
    "\n",
    "#new_list = find['type_find'].fillna('Unbekannt').apply(lambda x:find_conversion[x])\n",
    "find['type_find'] = new_list\n",
    "\n",
    "for row, i in enumerate(find['type_find']):\n",
    "    if find['num_coins'].loc[row] == 1 and i != ('Single Find'or'Hoard Find'):\n",
    "        find['type_find'].loc[row] = 'Single Find'\n",
    "    elif find['num_coins'].loc[row] > 1 and i != ('Single Find'or'Hoard Find'):\n",
    "        find['type_find'].loc[row] = 'Hoard Find'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds df is missing certainty values, so we add them here too\n",
    "# This is not something AFE provides insight on, so we assign our highest value\n",
    "find['cf_custom_region_vague'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d492d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the goal is to relink the finds to their component detailed coin finds\n",
    "# These have been agglomerated and lost in the above summing process\n",
    "# We just query the daip df by afe_find_id and put it into a new column\n",
    "our_finds = list(find['find_number'])\n",
    "biggest = max(find['num_coins'])\n",
    "all_coins = []\n",
    "# The loop builds a list of lists for each find (all_coins)\n",
    "for i in our_finds:\n",
    "    coins = []\n",
    "    for y in range(biggest):\n",
    "        try:\n",
    "            coin = daip[daip['afe_find_id']==i]['ID_x'].values[y]\n",
    "            coins.append(coin)\n",
    "        except:\n",
    "            continue\n",
    "    all_coins.append(coins)\n",
    "# The new column is made from the all_coins list of lists  \n",
    "find['detailed_find_ids'] = all_coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b07816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we make a function to take all those coins indexed to finds and turn them into URLs\n",
    "# These are mapped onto a new column, group_refs\n",
    "# That is then concated with references\n",
    "def det_fin_adder(*list_finds):\n",
    "    find_citations = ''\n",
    "    for i in list_finds:\n",
    "        for y in i:\n",
    "            find_citations = find_citations + ' || http://afe.dainst.org/detailedresult?l=en&link=' + str(y)\n",
    "    return find_citations\n",
    "\n",
    "find['group_refs'] = find['detailed_find_ids'].map(det_fin_adder)\n",
    "find['references'] = find['references'] + find['group_refs']\n",
    "\n",
    "# Then we drop the previous columns in favour of just references\n",
    "find = find.drop(columns=['detailed_find_ids','group_refs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "find.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "find.to_csv(path_or_buf='final_afe_finds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a4886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
