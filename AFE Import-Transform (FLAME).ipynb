{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8789bf4c",
   "metadata": {},
   "source": [
    "# Scraping Code for AFE\n",
    "The code below scrapes both the places and finds from the AFE website. afe_places require Selenium to work, is simpler, but is finicky in terms of start up. The finds reads the bare HTML, but due to variation in HTML templates for different kinds of finds, it involves a multipart conditional and is consequently rather long.\n",
    "\n",
    "Strictly speaking, scraping would not be at all needed, since the DAI-AFE website offers a CSV export of its data that exports everything in one go, provided you search without any parameters. The problem is that, while this export provides the find numbers, it does not provide place or hoard numbers, and so prevents the user from re-assembling the data. The virtue of the scraping procedure below is that it also gets those particular numbers.\n",
    "\n",
    "And THIS might not have been a problem either, since one could join on distinct strings and get to the numbers the other way around. But there are doubles in the afe_places df, such that a coin from that place would not know which entry to choose based on the provided value. These all have two entries in the places df: \n",
    "\n",
    "Gudensberg  \n",
    "Karlstadt   \n",
    "Vockerode   \n",
    "Berghofen   \n",
    "Dissen      \n",
    "Hamm        \n",
    "Haffen      \n",
    "Poppenhausen\n",
    "Stockum     \n",
    "Limburg       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371d3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are here in case you are debugging the scrapers. They are re-imported in the data cleaning section.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6295ad06",
   "metadata": {},
   "source": [
    "## The scrapers\n",
    "The import libraries below are common more or less for all the scrapers. Two of three use Selenium. Detailed finds doesn't require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc54e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import io\n",
    "import ast\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen, Request\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def setting_up(sales_url):\n",
    "  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "  req = Request(url=sales_url, headers=headers) \n",
    "  html = urlopen(req).read().decode('utf-8')\n",
    "  soup = BeautifulSoup(urlopen(req).read())\n",
    "  return soup\n",
    "\n",
    "#!pip install selenium\n",
    "#!apt-get update # to update ubuntu to correctly run apt install\n",
    "#!apt install chromium-chromedriver\n",
    "#!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "import sys\n",
    "\n",
    "#!pip install webdriver_manager\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#sys.path.insert(0,'/opt/homebrew/Caskroom/chromedriver')\n",
    "#from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59e0ce",
   "metadata": {},
   "source": [
    "### Place Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f930db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code works to grab what is needed from the places website using Selenium\n",
    "# In addition to Lee's code, I have added a tester that continues the loop\n",
    "# in cases of failure. \n",
    "# After every loop, I am also outputing a new copy of the csv, which will help with\n",
    "# debug in cases where this fails, say, 1500 rows in.\n",
    "\n",
    "for i in range(1, 10):\n",
    "    link = 'http://afe.dainst.org/place?afeid=' + str(i)\n",
    "\n",
    "    try:\n",
    "        driver.maximize_window()\n",
    "        driver.implicitly_wait(1000)\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(1000)\n",
    "        driver.find_element(By.CLASS_NAME,'lclinks')\n",
    "\n",
    "        html = driver.page_source\n",
    "        #print(pd.read_html(html)[0])\n",
    "        places_temp = pd.read_html(html)[0].set_index('Name')\n",
    "        #print(places_temp.loc['ID'][0])\n",
    "\n",
    "        new_line = [places_temp.loc['ID'][0], places_temp.loc['Name'][0],places_temp.loc['Längen- und Breitengrad'][0].split(',')[0],places_temp.loc['Längen- und Breitengrad'][0].split(',')[1]]\n",
    "        places.loc[i] = new_line\n",
    "        places.to_csv('afe_places.csv', index=False)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9470540",
   "metadata": {},
   "source": [
    "### \"Detailed Result\" Scraper\n",
    "This scrapes what is essentially the report of individual coins. This is where the majority of our information comes from. It is a difficult scrape. We aren't using Selenium, but instead just targeting text patterns and grabbing whatever comes up in them. This results in some incredibly messy columns, occasionally grabbing the entire HTML text and sticking it into individual columns. I have, through Regex, tried to tame these. The occasional outlier gets through, even by the end, which you will see me weed out of the results in mint conversion stage of data cleaning (not even in this section... way down the line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ff15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works, and covers a number of different variations in the HTML template used to generate pages\n",
    "# So far used on #1-5000\n",
    "\n",
    "start_id = 15001\n",
    "end_id = 18000\n",
    "\n",
    "cols = ['ID', 'place_id', 'afe_find_id', 'Location', 'Status', 'Denomination', 'Issuer', 'Mint', 'Date', 'References', 'Remarks', 'Bibliography']\n",
    "AFE = pd.DataFrame(columns=cols)\n",
    "\n",
    "while start_id <= end_id:\n",
    "    url = 'http://afe.dainst.org/detailedresult?l=en&link=' + str(start_id)\n",
    "    links = setting_up(url).find_all('a')\n",
    "    place_link = str(links[0])\n",
    "    blank = '<a href=\"http://afe.dainst.org/coin?afeid=&amp;l=en\" target=\"_blank\">http://afe.dainst.org/coin?afeid=</a>'\n",
    "    \n",
    "    # A big problem in the AFE implementation is that links come in various orders.\n",
    "    # We had solved this by accounting for fixed places in which links came\n",
    "    # But these turned out not to be so fixed. So I've put in a simple \n",
    "    # loop that determines the right link for each entry. This should fix \n",
    "    # \"invalid literal\" errors that pop up when you pass the wrong formatted link\n",
    "    # down the pipeline\n",
    "\n",
    "    \n",
    "    image = '.jpg'\n",
    "    if place_link == blank:\n",
    "        start_id += 1\n",
    "        print(start_id)\n",
    "        time.sleep(0.2)\n",
    "        continue\n",
    "    elif image in place_link:\n",
    "        # The link number differs for image pages than not (say AFE 4 or 5). This is one difference.\n",
    "        #place_link = str(links[2])\n",
    "        #print(place_link)\n",
    "        \n",
    "        marker = \"place?afeid=\"\n",
    "        for i in links:\n",
    "            if marker in str(i):\n",
    "                place_link = str(i)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        temp = setting_up(url).get_text()\n",
    "        #print(temp)\n",
    "\n",
    "        loc = temp.find('Location')\n",
    "        sta = temp.find('Status')\n",
    "        deno = temp.find('Denomination')\n",
    "        iss = temp.find('Issuer')\n",
    "        mint = temp.find('Mint')\n",
    "        dat = temp.find('Date')\n",
    "        ref = temp.find('References')\n",
    "        rem = temp.find('Remarks')\n",
    "        bib = temp.find('Bibliography')\n",
    "        end_text = temp.find('Export CSV')-22\n",
    "        #print(loc,sta,deno,iss,mint,dat,ref,rem,bib,end_text)\n",
    "\n",
    "        # One major problem is that various fields wind up being empty in the AFE. When they are\n",
    "        # It breaks the code, since the fields simply disappear. These produce a \"-1\" value when\n",
    "        # Evaluatd by the above \"find\" functions. The code below is a diagnostic to show which fields \n",
    "        # Are empty\n",
    "        #fields = [loc,sta,deno,iss,mint,dat,ref,rem,bib,end_text]\n",
    "\n",
    "        #for n, i in enumerate(fields):\n",
    "        #    if i == -1:\n",
    "        #        fields[n] = 'empty'\n",
    "        #    else:\n",
    "        #        continue\n",
    "\n",
    "\n",
    "        # The problem with image pages is that some have status fields and others do not. It is inconsistent.\n",
    "        # so we need to build in a conditional for this. this is done for each variable below, where it is \n",
    "        # known to be a problem\n",
    "        if sta != -1:\n",
    "            loc_string = temp[loc+8:sta-1]\n",
    "            sta_string = temp[sta+6:deno]\n",
    "        else:\n",
    "            loc_string = temp[loc+8:deno]\n",
    "            sta_string = 'No status'\n",
    "\n",
    "        deno_string = temp[deno+12:iss]\n",
    "\n",
    "        if temp.find('Issued for') > 0: iss_string = temp[iss+6:temp.find('Issued for')]\n",
    "        else: iss_string = temp[iss+6:mint]\n",
    "\n",
    "        min_string = temp[mint+4:dat]\n",
    "\n",
    "        if ref > 0: dat_string = temp[dat+4:ref]\n",
    "        else: dat_string = temp[dat+4:rem]\n",
    "\n",
    "        if (temp.find('Obv./Rev.') > 0) and (ref > 0): ref_string = temp[ref+10:temp.find('Obv./Rev.')]\n",
    "        elif (temp.find('Obv./Rev.') == -1): ref_string = ''\n",
    "        elif (ref > 0): ref_string = temp[ref+10:rem]\n",
    "        else: ref_string = 'No reference'\n",
    "\n",
    "        if bib > 0: \n",
    "            rem_string = temp[rem+7:bib]\n",
    "        elif bib == -1: \n",
    "            rem_string = 'No remarks'\n",
    "\n",
    "        bib_string = temp[bib+12:end_text]\n",
    "        #print(loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string, sep='; ')\n",
    "\n",
    "        #print(setting_up(url).find('#place?afeid='))\n",
    "        #links = setting_up(url).find_all('a')\n",
    "\n",
    "        place_start = place_link.find('place?afeid=') + 12\n",
    "        place_end = place_link.find('\" tar')\n",
    "        #print(place_start, place_end)\n",
    "\n",
    "        place_id = int(place_link[place_start:place_end])\n",
    "\n",
    "        #The same problem above is replicated in the find_spot link position.\n",
    "        #Also created an \"invalid literal\" error\n",
    "        find_marker = \"findspot?afeid=\"\n",
    "        for i in links:\n",
    "            if find_marker in str(i):\n",
    "                find_link = str(i)\n",
    "            else:\n",
    "                continue\n",
    "        #find_link = str(links[2])\n",
    "        link_start = find_link.find('findspot?afeid=') + 15\n",
    "        link_end = find_link.find('\" tar')\n",
    "        link_id = int(find_link[link_start:link_end])\n",
    "\n",
    "\n",
    "        #print(place_id, link_id, sep='; ')\n",
    "        new_line = [start_id, place_id, link_id, loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string]\n",
    "        AFE.loc[start_id] = new_line\n",
    "\n",
    "        start_id += 1\n",
    "        print(start_id)\n",
    "        time.sleep(0.2)\n",
    "        continue\n",
    "    else:\n",
    "        #url = 'http://afe.dainst.org/detailedresult?l=en&link=' + str(start_id)\n",
    "        temp = setting_up(url).get_text()\n",
    "        loc = temp.find('Location')\n",
    "        sta = temp.find('Status')\n",
    "        deno = temp.find('Denomination')\n",
    "        iss = temp.find('Issuer')\n",
    "        mint = temp.find('Mint')\n",
    "        dat = temp.find('Date')\n",
    "        ref = temp.find('References')\n",
    "        rem = temp.find('Remarks')\n",
    "        bib = temp.find('Bibliography')\n",
    "        end_text = temp.find('Export CSV')-22\n",
    "\n",
    "        loc_string = temp[loc+8:sta-1]\n",
    "        sta_string = temp[sta+6:deno]\n",
    "        deno_string = temp[deno+12:iss]\n",
    "        if temp.find('Issued for') > 0: iss_string = temp[iss+6:temp.find('Issued for')]\n",
    "        else: iss_string = temp[iss+6:mint]\n",
    "        min_string = temp[mint+4:dat]\n",
    "        if ref > 0: dat_string = temp[dat+4:ref]\n",
    "        else: dat_string = temp[dat+4:rem]\n",
    "        if (temp.find('Obv./Rev.') > 0) and (ref > 0): ref_string = temp[ref+10:temp.find('Obv./Rev.')]\n",
    "        elif (ref > 0): ref_string = temp[ref+10:rem]\n",
    "        else: ref_string = ''\n",
    "        rem_string = temp[rem+7:bib]\n",
    "        bib_string = temp[bib+12:end_text]\n",
    "        #print(loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string, sep='; ')\n",
    "\n",
    "\n",
    "        #print(setting_up(url).find('#place?afeid='))\n",
    "        links = setting_up(url).find_all('a')\n",
    "        #place_link = str(links[0])\n",
    "        place_start = place_link.find('place?afeid=') + 12\n",
    "        place_end = place_link.find('\" tar')\n",
    "        place_id = int(place_link[place_start:place_end])\n",
    "\n",
    "        find_link = str(links[1])\n",
    "        link_start = find_link.find('findspot?afeid=') + 15\n",
    "        link_end = find_link.find('\" tar')\n",
    "        link_id = int(find_link[link_start:link_end])\n",
    "\n",
    "\n",
    "        #print(place_id, link_id, sep='; ')\n",
    "        new_line = [start_id, place_id, link_id, loc_string, sta_string, deno_string, iss_string, min_string, dat_string, ref_string, rem_string, bib_string]\n",
    "        AFE.loc[start_id] = new_line\n",
    "\n",
    "        start_id += 1\n",
    "        time.sleep(0.2)\n",
    "        print(start_id)\n",
    "AFE.to_csv('afe_detailed_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e673d92",
   "metadata": {},
   "source": [
    "### Find Scraper (Fund Ort)\n",
    "This one was pretty simple. Another instance of Selenium scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5115778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the previous scraping is not sufficient because we are missing find categories (hoards, etc.)\n",
    "# We need to scrape the fundort pages too\n",
    "# This requires Selenium :-(\n",
    "\n",
    "# This code works to grab what is needed from the places website using Selenium\n",
    "# In addition to Lee's code, I have added a tester that continues the loop\n",
    "# in cases of failure. \n",
    "# After every loop, I am also outputing a new copy of the csv, which will help with\n",
    "# debug in cases where this fails, say, 1500 rows in.\n",
    "#!pip install webdriver_manager\n",
    "\n",
    "# We ran into some problems, and it had to do with the reset timing on the Selenium browser. \n",
    "# I built a test function that basically told the program to continue a loop if it did not \n",
    "# resolve within 5 seconds (thus getting past non-conforming URLs). \n",
    "# But the browser had to be reset for the next step too---except the browser auto-reset \n",
    "# was 1000 seconds and nothing about our reset test triggered this. \n",
    "# Thus all future loops resolved without any input from the browsers (they were all failed loops). \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# There are a lot of empty spaces in the finds database.\n",
    "# They take a long time to resolve, so any find that takes longer than\n",
    "# A certain amount of time is an error. We build in a timer to resolve\n",
    "# These errors\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "# You have to create the DF in advance, because if the try function fails, subsequent\n",
    "# attempts, including in except will have no df to latch onto.\n",
    "# Lee did not need this in the places scrape, since there is a successful place at \n",
    "# loc #1. There is not one here.\n",
    "data = [['','','','']]\n",
    "finds = pd.DataFrame(data, columns=['ID', 'Name', 'Fundkategorie', 'Link'])        \n",
    "\n",
    "for i in range(1,4018):\n",
    "    link = 'http://afe.dainst.org/findspot?afeid=' + str(i)\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(4)\n",
    "    driver.get(link)\n",
    "    driver.implicitly_wait(4)\n",
    "\n",
    "    \n",
    "    #print(pd.read_html(html)[0])\n",
    "    try:\n",
    "        with time_limit(5):\n",
    "            driver.find_element(By.CLASS_NAME,'lclinks')\n",
    "\n",
    "            html = driver.page_source\n",
    "\n",
    "            finds_temp = pd.read_html(html)[0].set_index('Name')\n",
    "            #print(places_temp.loc['ID'][0])\n",
    "\n",
    "            new_line = [finds_temp.loc['ID'][0], finds_temp.loc['Name'][0],finds_temp.loc['Fundkategorie'][0],finds_temp.loc['Link'][0]]\n",
    "            finds.loc[i] = new_line\n",
    "            print(i, \"Success!\")\n",
    "            finds.to_csv('afe_fundort.csv', index=False)\n",
    "    except:\n",
    "        new_line = ['', '','','']\n",
    "        finds.loc[i] = new_line\n",
    "        print(i, \"Empty\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd505ba",
   "metadata": {},
   "source": [
    "# Converting Scraped Data to Our Format\n",
    "We start with the Detailed Finds report and work off of that, as that is the main source of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d8839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/n5w99lgd16773rvsbzs4mmdh0000gn/T/ipykernel_5986/694732682.py:11: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  dai['Location'] = dai['Location'].str.replace('(','')\n",
      "/var/folders/j3/n5w99lgd16773rvsbzs4mmdh0000gn/T/ipykernel_5986/694732682.py:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  dai['Location'] = dai['Location'].str.replace(')','')\n",
      "/var/folders/j3/n5w99lgd16773rvsbzs4mmdh0000gn/T/ipykernel_5986/694732682.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  dai['Location'] = dai['Location'].str.replace('?','')\n",
      "/opt/homebrew/lib/python3.9/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 5000) \n",
    "pd.set_option('display.max_rows', 5000)\n",
    "dai = pd.read_csv('afe_detailed_results.csv')\n",
    "\n",
    "# Some elements messing up the map\n",
    "dai['Location'] = dai['Location'].str.replace('(','')\n",
    "dai['Location'] = dai['Location'].str.replace(')','')\n",
    "dai['Location'] = dai['Location'].str.replace('?','')\n",
    "dai['Location'] = dai['Location'].str.replace('\"','')\n",
    "\n",
    "# Replace NaNs in the YearFound column with 1700, since blank years prevents those\n",
    "# rows from being summed along with the other ones\n",
    "\n",
    "#dai['YearFound'] = dai['YearFound'].fillna(1700)\n",
    "dai['Status'] = dai['Status'].fillna('Einzelfund_FC')\n",
    "\n",
    "# There are many values in dai['Date'] that simply won't yield a date range. So we\n",
    "# Are doing some Regex work here.\n",
    "\n",
    "# This is a an error that occurs when nothing is captured for the Date value\n",
    "# It throws in the entire HTML output of the scraper -- not useful\n",
    "# We replace that with \"malformed\"\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"[.|\\n|\\W|\\w]*Detailed Result[.|\\n|\\W|\\w]*\", \"Malformed\", i)\n",
    "\n",
    "# This replaces all '\\n' entries in bad values that do not conform to the above\n",
    "# The '\\n' interferes with identifying date ranges, setting up a succesful\n",
    "# Targeting operation in the next loop\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(.*)(\\\\n)(.*)\", \"\\\\1\\\\3\", i)\n",
    "\n",
    "# Need to replace 'bis' with '-'\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"bis\", \"-\", i)\n",
    "\n",
    "# What follows is a pretty straightforward series of transformations of bad formats\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(LT)(.*)\", \"Unclear Periodization\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(nach\\s)(\\d+)\", \"\\\\2-750\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(\\()(\\d+-\\d+)\", \"\\\\2\", i)\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"Zeitstellung unbekannt*\", \"Date Unknown\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c42716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop deals exclusively with transforming century ranges into hard date ranges\n",
    "# It relies exlusively on re\n",
    "\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    # there are a few common kinds of tags for dross. Everything after these keywords is garbage\n",
    "    entry = re.sub(\"(.*)(Bibliography)(.*)\", \"\\\\1\", i)\n",
    "    entry = re.sub(\"(.*)(Secondary)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Weight)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Obv)(.*)\", \"\\\\1\", entry)\n",
    "    entry = re.sub(\"(.*)(Peculiarities)(.*)\", \"\\\\1\", entry)\n",
    "    # After this, we have some conditionals. These account for differences in form btw\n",
    "    # Single century dates, as well as BCE dates (those with only BCE are cut entirely, and\n",
    "    # those that straddle BCE/CE are assigned a 1-??? date range)\n",
    "\n",
    "    if re.search(\"Jh.|Jahrhundert|\\d+\\.\", i) != None:\n",
    "        try:\n",
    "            # The len(???) conditional assesses how many numbers are present in a field.\n",
    "            # If 2, then it is a range, if 1 then not\n",
    "            if len(re. findall(\"\\d+\", entry)) == 1 and re.search(\"v\\.\\sChr\\.\", i) != None:\n",
    "                century_start = '0'\n",
    "                century_end = '0'\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 1:\n",
    "                century = re.sub(\"(.*)(\\d+)(.*)\", \"\\\\2\", entry)\n",
    "                century_start = int(century) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"Jahrhundert|Jh\", i) == None:\n",
    "                century_start = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", entry)\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", entry)\n",
    "                century_start = int(century_start) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"v\\.\\sChr\\.\", i) == None:\n",
    "                century_start = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", entry)\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", entry)\n",
    "                century_start = int(century_start) - 1\n",
    "                century_start = str(century_start) + \"00\"\n",
    "                if century_start == \"000\":\n",
    "                    century_start = \"1\"\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            elif len(re. findall(\"\\d+\", entry)) == 2 and re.search(\"v\\.\\sChr\\.\", i) != None and re.search(\"n\\.\\sChr\\.\", i) != None:\n",
    "                century_start = '1'\n",
    "                century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "                century_end = int(century_end) - 1\n",
    "                century_end = str(century_end) + \"99\"\n",
    "            else:\n",
    "                century_start = \"0\"\n",
    "                century_end = \"0\"\n",
    "\n",
    "            if century_end == '099':\n",
    "                century_end = '99'\n",
    "\n",
    "            date = century_start + \"-\" + century_end\n",
    "            dai['Date'].loc[row] = date\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd97bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is looking for any lone century dates (e.g., 1. Jh.) by looking for 'Jh.' within a certain number\n",
    "# of characters of 1 (or whatever digit it may be). It seems to work pretty well.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if re.search(\"(\\d+)(\\.)(?![.!?] )\\W+(?:\\w+(?![.!?] )\\W+){1,2}?(Jh.){1,2}?(?![.!?] )\\W+(?:\\w+(?![.!?] )\\W+)\", i) != None:\n",
    "        print(row, i)\n",
    "        century = re.sub(\"(\\d+)(\\.)(.*)\", \"\\\\1\", i)\n",
    "        century = int(century) - 1\n",
    "        century = str(century) + '00'\n",
    "        if century == '000':\n",
    "            century = '1'\n",
    "        print(century)\n",
    "\n",
    "# Final BCE/CE cleanup, now not just on centuries\n",
    "# This loop gets rid of anything remaining that straddles BCE/CE since this is causing problems\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 2 and re.search(\"v\\.\", i) != None and re.search(\"n\\.\", i) != None:\n",
    "        century_start = '1'\n",
    "        century_end = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "        date = century_start + \"-\" + century_end\n",
    "        dai['Date'].loc[row] = date\n",
    "\n",
    "# And single dates in BCE are eliminated\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 1 and re.search(\"v\\.\", i) != None:\n",
    "        dai['Date'].loc[row] = '0'\n",
    "\n",
    "# All date ranges that are BCE only\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re.findall(\"\\d+\", i)) == 2 and re.search(\"v\\.\", i) != None and re.search(\"n\\.\", i) == None:\n",
    "        dai['Date'].loc[row] = '0'\n",
    "\n",
    "# This one, having solved the '\\n's actually replaces everything but the date range\n",
    "# It works surprisingly well. Previously I ran this at the start of the filtering\n",
    "# sequence, but it turns out to work much better if it comes at the very end.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(\\d+-\\d+|\\d+)(.*)\", \"\\\\1\", i)\n",
    "\n",
    "# The one category of string artifact that seems to get through the previous round of re filters \n",
    "# and messes with the next bit of code in the process, is '.ca' (and 'Ende', as it turns out), \n",
    "# so these are removed at this stage\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    dai['Date'].loc[row] = re.sub(\"(.*)ca.\\s(.*)|(.*)(Ende\\s)(.*)\", \"\\\\2\", i)\n",
    "\n",
    "# This loop deals with an artifact created by previous regex steps. Getting the regex order is finnicky so it applies\n",
    "# to 99% of cases. It produces these strings where 00 and 99 have been added to normal date ranges\n",
    "# It thinks they are centuries. Anyway, the easiest solution is to ID these and to correct them by \n",
    "# subtracting 2 characters from the end of the string. This does that.\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re. findall(\"\\d+\", i)) == 2:\n",
    "        first = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", i)\n",
    "        second = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "        if len(first) > 3:\n",
    "            first = first[:-2]\n",
    "            # A previous step seems to -1 these (but not other ranges), so I correct here\n",
    "            first = int(first) + 1\n",
    "            first = str(first)\n",
    "        if len(second) > 3:\n",
    "            second = second[:-2]\n",
    "            second = int(second) + 1\n",
    "            second = str(second)\n",
    "        newrange = first+'-'+second\n",
    "        dai['Date'].loc[row] = newrange\n",
    "\n",
    "# Create two columns from the Date column\n",
    "dai['Date Min'] = ''\n",
    "dai['Date Max'] = ''\n",
    "\n",
    "# This separates dateranges that remain into useful elements for date min and max\n",
    "# the len business below is simply to see how many dates we are working with\n",
    "# So len 1 means there is just a single year, len 0 means there is not a meaningful year\n",
    "# and len 2 is a standard date range\n",
    "for row, i in enumerate(dai['Date']):\n",
    "    if len(re. findall(\"\\d+\", i)) == 0:\n",
    "        dai['Date Min'].loc[row] = ''\n",
    "        dai['Date Max'].loc[row] = ''\n",
    "    if len(re. findall(\"\\d+\", i)) == 1:\n",
    "        dai['Date Min'].loc[row] = i\n",
    "        dai['Date Max'].loc[row] = i\n",
    "    if len(re. findall(\"\\d+\", i)) == 2:\n",
    "        dai['Date Min'].loc[row] = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\1\", i)\n",
    "        dai['Date Max'].loc[row] = re.sub(\"(\\d+)(.*?)(\\d+)(.*)\", \"\\\\3\", i)\n",
    "\n",
    "# We have some detritus that will prevent a successful conversion of the column to int\n",
    "# This removes that\n",
    "for row, i in enumerate(dai['Date Min']):\n",
    "    if re.findall(\"(vor\\s|\\(|-)(\\d+)\", i) != None:\n",
    "        dai['Date Min'].loc[row] = re.sub(\"(vor\\s|\\(|-)(\\d+)\", \"\\\\2\", i)\n",
    "\n",
    "for row, i in enumerate(dai['Date Max']):\n",
    "    if re.findall(\"(vor\\s|\\(|-)(\\d+)\", i) != None:\n",
    "        dai['Date Max'].loc[row] = re.sub(\"(vor\\s|\\(|-)(\\d+)\", \"\\\\2\", i)\n",
    "\n",
    "# The dates in this row somehow survived everything and generated a bad MinMax Date range\n",
    "# Just drop it here\n",
    "dai.drop(index=dai[dai['ID'] == 16989].index, inplace=True)\n",
    "dai.drop(index=dai[dai['Date Min'] == ''].index, inplace=True)\n",
    "dai.drop(index=dai[dai['Date Min'] == '0'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebfcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotta reset index to facilitate looping\n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0787c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of strings in these columns to integers\n",
    "dai = dai.astype({\"Date Min\": int, \"Date Max\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045bf7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some final weeding out of BCE related entries\n",
    "# First by getting rid of anything where DateMin is greater than DateMax\n",
    "for row, i in enumerate(dai['Date Min']):\n",
    "    if i > dai['Date Max'].loc[row]:\n",
    "        dai.drop([row], inplace=True)\n",
    "        \n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04de2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then removing anything with Republik under ruler\n",
    "dai = dai[dai['Issuer'] != 'Republik']\n",
    "dai = dai[dai['Date Min'] > 324]\n",
    "dai = dai[dai['Date Max'] < 751]\n",
    "\n",
    "\n",
    "# There is a strange phenomenon where 4 entries of Roman emperors have date ranges transposed,\n",
    "# appearing to be much later than they are (e.g., 600-699). I simply remove them here\n",
    "# esp since none would fall into our date range\n",
    "dai = dai[dai['Issuer'] != 'Antoninus Pius']\n",
    "dai = dai[dai['Issuer'] != 'Domitianus']\n",
    "dai = dai[dai['Issuer'] != 'Commodus']\n",
    "dai = dai[dai['Issuer'] != 'Nerva']\n",
    "dai.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d038ed",
   "metadata": {},
   "source": [
    "## Linking to other datasets and producing group/find dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a814ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here we will link Place to Find information, specifically for geogrpahic coords\n",
    "places = pd.read_csv('afe_places.csv')\n",
    "places = places.rename(columns={\"ID\": \"place_id\"})\n",
    "#places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ada671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dai (output of last section) with places. produces daip (though I suppose I could keep \n",
    "# the name as dai---oh well.)\n",
    "daip = pd.merge(dai, places,  how='left', left_on=['place_id'], right_on = ['place_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e124aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import the Fund Ort df in order to get type_find\n",
    "# then we merge with existing daip\n",
    "fundort = pd.read_csv('afe_fundort.csv')\n",
    "daip = pd.merge(daip, fundort,  how='left', left_on=['afe_find_id'], right_on = ['ID'])\n",
    "daip = daip.drop(['ID_y', 'Name', 'Link'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647593a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We begin from detailed finds df, and implicitly at this level of ontology\n",
    "# we are dealing with single coins. We will be summing these to produce finds\n",
    "# at that point the qunatities will turn into aggregates\n",
    "# So this creates those quantities\n",
    "daip['Quantity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a28226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the goal is to create groups by summing the Quantity column along multiple indexes\n",
    "group = daip.groupby(['Location','afe_find_id','Mint','Denomination','Date Min','Date Max','Issuer'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "# The process yields a Pandas series. That series should be turned into a df\n",
    "group = group.to_frame()\n",
    "# That df needs to level out the indexes back into normal values.\n",
    "# Each index value simply becomes a repeated value in the relevant cells\n",
    "group = group.reset_index(level=['Location','afe_find_id','Date Min', 'Date Max', 'Denomination','Mint','Issuer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a1008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create distinct group numbers. This covnerts the df index into that.\n",
    "group = group.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25393926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# This turns those group numbers into display labels for the map\n",
    "group['coin_group_id'] = ''\n",
    "for row, i in enumerate(group.index):\n",
    "    group['coin_group_id'].loc[row] = 'AFE-' + str(i +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219749e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to rename a bunch of these columns\n",
    "group = group.rename(columns={\"index\": \"group_id\", \"\" \"Location\": \"name\", \"Issuer\" : \"ruler\", \"Denomination\" : \"denomination\",\\\n",
    "                     \"Quantity\" : \"num_coins\", \"Mint\" : \"mint\", \"Date Min\" : \"start_year\", \"Date Max\" : \"end_year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b272343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there are some columns where values need to be added. I am keeping the excavation year column blank for now\n",
    "today = datetime.date.today()\n",
    "group['created'] = today.strftime(\"%m-%d-%Y\")\n",
    "group['imported'] = today.strftime(\"%m-%d-%Y\")\n",
    "group['owner'] = 'DAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54da596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to convert denominations to our format before we assign metals\n",
    "# This was taken almost wholesale from Lee's CHRE code\n",
    "\n",
    "# For future reference, to find all denoms\n",
    "# group.denomination.unique()\n",
    "\n",
    "denomination_conversion = {\n",
    "    'Denarius':'denarius', 'Semis':'semissis',\n",
    "       'Aureus':'aureus', 'AE':'uncertain (bronze)', 'Antoninianus':'radiate or nummus (UK find)',\n",
    "       'Follis':'follis','AE2':'AE2 (5.15g)', 'Drachme':'drachm', 'AV':'AV', 'Maiorina':'follis',\n",
    "       'AE3':'AE3 (2.58g)', 'Solidus':'solidus', 'Silber':'uncertain (silver)', 'Siliqua':'siliqua', \n",
    "    'AE4':'AE4 (1.23g)', 'Tremissis':'tremissis', '10 Num' : '10 nummi', '2 Solidi' : '2 solidi',\n",
    "    'Siliqua (reduziert)' : 'reduced siliqua', 'Miliarensis' : 'miliarensis'\n",
    "}\n",
    "\n",
    "obsolete_denominations = ['Sestertius', 'As', 'Tetradrachme', 'Centenionalis', 'Dupondius',  'Quadrans', 'Doppelsestertius', 'Dupondius / As']\n",
    "\n",
    "# actual conversion of denominations to FLAME style\n",
    "group = group[~group['denomination'].isin(obsolete_denominations)]\n",
    "new_list = group['denomination'].fillna('Uncertain').apply(lambda x:denomination_conversion[x])\n",
    "group['denomination'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e2f1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert denominations to metal\n",
    "# Generate metal\n",
    "metal_conversion = {\n",
    "   'denarius':'silver',\n",
    "    'semissis':'gold',\n",
    "    'aureus':'gold',\n",
    "    'uncertain (bronze)':'bronze',\n",
    "    'radiate or nummus (UK find)':'bronze',\n",
    "    'follis':'bronze', \n",
    "    'AE2 (5.15g)':'bronze',\n",
    "    'drachm':'silver',\n",
    "    'AV':'gold',\n",
    "    'AE3 (2.58g)':'bronze',\n",
    "    'solidus':'gold',\n",
    "    'uncertain (silver)':'silver',\n",
    "    'siliqua':'silver',\n",
    "    'AE4 (1.23g)':'bronze',\n",
    "    'tremissis':'silver',\n",
    "    '10 nummi':'bronze',\n",
    "    '2 solidi':'gold',\n",
    "    'reduced siliqua':'silver',\n",
    "    'miliarensis':'silver'\n",
    "}\n",
    "\n",
    "other_metals = []\n",
    "group['metal'] = \"\"\n",
    "group = group[~group['metal'].isin(other_metals)]\n",
    "new_list = group['denomination'].fillna('Uncertain').apply(lambda x:metal_conversion[x])\n",
    "group['metal'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1999278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to convert mints.\n",
    "#group.mint.unique()\n",
    "mint_conversion = {\n",
    "    'Roma':'Roma', 'Alexandria':'Alexandria ad Aegyptum', 'Uncertain mint':'Unknown', 'Constantinopolis':'Constantinople',\n",
    "       'Unofficial mint':'Unknown', 'Siscia':'Siscia', 'Thessalonica':'Thessalonika', 'Londinium':'Londinium',\n",
    "       'Treveri':'Colonia Augusta Treverorum', 'Lugdunum':'Lugdunensium', 'Ticinum':'Ticinum', 'Aquileia':'Aquileia', 'Colonia CAA':'Unknown',\n",
    "       'Antiochia':'Antioch', 'Emerita':'Emerita', 'Lycia':'Unknown (East Roman)', 'Cyzicus':'Kyzikos', \n",
    "    'Roma / Lugdunum':'Roma or Lugdunum', #new mint\n",
    "       'Sirmium':'Sirmium', 'Eastern mint':'Unknown (East Roman)', 'Gallia':'Unknown (Gaul)', 'Unidentified mint':'Unknown',\n",
    "       'Laodicea ad Mare':'Laodicea ad Mare', #new mint\n",
    "    'Mediolanum':'Mediolanum', 'Ravenna':'Ravenna', 'Roma / Tarraco (?)':'Roma or Tarracona', #new mint\n",
    "       'Africa':'Unknown (Africa)', #new mint\n",
    "    'Hispania':'Unknown (Iberia)', 'Colonia Caesaraugusta':'Cesaraugusta', 'Greek East':'Unknown (East Roman)',\n",
    "    'Münzstätte nicht bekannt':'Unknown (Germany)', 'Östliche Münzstätte' : 'Unknown (Germany)',\n",
    "    '\\n\\n\\n\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nID: 11443\\n\\n\\nName\\nValue\\n\\n\\nID11443LocationBad Sulza, Bad SulzaStatusOfficialDenominationFollisIssuerConstans':'Bad Sulza',\n",
    "    '\\n\\n\\n\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nID: 11444\\n\\n\\nName\\nValue\\n\\n\\nID11444LocationBad Sulza, Bad SulzaStatusOfficialDenominationFollisIssuerConstantius II.':'Bad Sulza',\n",
    "    'Irreguläre Münzstätte' : 'Unknown (Germany)', 'Sicilia' : 'Sicily', 'Syrien' : 'Unknown (Greater Syria)',\n",
    "    'Arelate' : 'Arelato', 'Heracleia' : 'Heraclea'\n",
    "}\n",
    "\n",
    "obsolete_mints = ['Nicomedia']\n",
    "\n",
    "group = group[~group['mint'].isin(obsolete_mints)]\n",
    "new_list = group['mint'].fillna('Uncertain').apply(lambda x:mint_conversion[x])\n",
    "group['mint'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f2ec4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final regex cleanup of some ruler names, which went through the original regex process (see stages above)\n",
    "# in the scraping and cleaning stages\n",
    "# And picked up some odd elements\n",
    "x = lambda a : re.sub('(.*)(Date)(.*)|(\\\\n.*)', '\\\\1', a)\n",
    "group['ruler'] = group['ruler'].map(x)\n",
    "\n",
    "\n",
    "\n",
    "#for i in group['ruler']:\n",
    "#    print(x(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d908ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.to_csv(path_or_buf='final_afe_groups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479b679",
   "metadata": {},
   "source": [
    "### Create Finds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89005be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving values NaN in any row of daip means that when you create a new df, they will\n",
    "# be dropped. You need to provide some value\n",
    "#daip[['Fundkategorie','References']] = daip[['Fundkategorie','References']].fillna('Unknown')\n",
    "daip['Fundkategorie'] = daip['Fundkategorie'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e97e3fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_x</th>\n",
       "      <th>place_id</th>\n",
       "      <th>afe_find_id</th>\n",
       "      <th>Location</th>\n",
       "      <th>Status</th>\n",
       "      <th>Denomination</th>\n",
       "      <th>Issuer</th>\n",
       "      <th>Mint</th>\n",
       "      <th>Date</th>\n",
       "      <th>References</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Bibliography</th>\n",
       "      <th>Date Min</th>\n",
       "      <th>Date Max</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>Fundkategorie</th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289</td>\n",
       "      <td>34</td>\n",
       "      <td>2259</td>\n",
       "      <td>Altkönig, Altkönig</td>\n",
       "      <td>No status</td>\n",
       "      <td>Siliqua</td>\n",
       "      <td>Constantinus III.</td>\n",
       "      <td>Lugdunum</td>\n",
       "      <td>407-408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nach Foto bestimmt.Weight1.6</td>\n",
       "      <td>Böhme, Altkönig, Der \"Altkönig\" im Taunus als ...</td>\n",
       "      <td>407</td>\n",
       "      <td>408</td>\n",
       "      <td>Altkönig</td>\n",
       "      <td>50.212</td>\n",
       "      <td>8.482</td>\n",
       "      <td>Einzelfund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3522</td>\n",
       "      <td>169</td>\n",
       "      <td>203</td>\n",
       "      <td>Laatzen, Laatzen Schatzfund</td>\n",
       "      <td>Official</td>\n",
       "      <td>Miliarensis</td>\n",
       "      <td>Constantius II.</td>\n",
       "      <td>Constantinopolis</td>\n",
       "      <td>351-355</td>\n",
       "      <td>Coh., 326 RIC 8, 100Mint mark-/-//C·AWeight4.5...</td>\n",
       "      <td>\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>FMRD VII, Die Fundmünzen der Römischen Zeit in...</td>\n",
       "      <td>351</td>\n",
       "      <td>355</td>\n",
       "      <td>Laatzen</td>\n",
       "      <td>52.315</td>\n",
       "      <td>9.797</td>\n",
       "      <td>Schatzfund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3523</td>\n",
       "      <td>169</td>\n",
       "      <td>203</td>\n",
       "      <td>Laatzen, Laatzen Schatzfund</td>\n",
       "      <td>Official</td>\n",
       "      <td>Siliqua (reduziert)</td>\n",
       "      <td>Constantius II.</td>\n",
       "      <td>Arelate</td>\n",
       "      <td>355-363</td>\n",
       "      <td>Coh., 343 RIC 8, 261/291Mint mark-/-//SCONWeig...</td>\n",
       "      <td>\\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>FMRD VII, Die Fundmünzen der Römischen Zeit in...</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>Laatzen</td>\n",
       "      <td>52.315</td>\n",
       "      <td>9.797</td>\n",
       "      <td>Schatzfund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3524</td>\n",
       "      <td>169</td>\n",
       "      <td>203</td>\n",
       "      <td>Laatzen, Laatzen Schatzfund</td>\n",
       "      <td>Imitation</td>\n",
       "      <td>Silber</td>\n",
       "      <td>Constantinus II.</td>\n",
       "      <td>Irreguläre Münzstätte</td>\n",
       "      <td>353-360</td>\n",
       "      <td>RIC 8, wie (Lug) 216/217 Coh., wie 341Secondar...</td>\n",
       "      <td>Von Berger als barbarische Nachahmung ausgewie...</td>\n",
       "      <td>FMRD VII, Die Fundmünzen der Römischen Zeit in...</td>\n",
       "      <td>353</td>\n",
       "      <td>360</td>\n",
       "      <td>Laatzen</td>\n",
       "      <td>52.315</td>\n",
       "      <td>9.797</td>\n",
       "      <td>Schatzfund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3525</td>\n",
       "      <td>169</td>\n",
       "      <td>203</td>\n",
       "      <td>Laatzen, Laatzen Schatzfund</td>\n",
       "      <td>Official</td>\n",
       "      <td>Miliarensis</td>\n",
       "      <td>Iulianus II.</td>\n",
       "      <td>Lugdunum</td>\n",
       "      <td>360-363</td>\n",
       "      <td>RIC 8, 209Mint mark-/-//LVG</td>\n",
       "      <td>Datierung nach RIC (abweichend vom FMRD-Band, ...</td>\n",
       "      <td>FMRD VII, Die Fundmünzen der Römischen Zeit in...</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "      <td>Laatzen</td>\n",
       "      <td>52.315</td>\n",
       "      <td>9.797</td>\n",
       "      <td>Schatzfund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_x  place_id  afe_find_id                     Location     Status  \\\n",
       "0   289        34         2259           Altkönig, Altkönig  No status   \n",
       "1  3522       169          203  Laatzen, Laatzen Schatzfund   Official   \n",
       "2  3523       169          203  Laatzen, Laatzen Schatzfund   Official   \n",
       "3  3524       169          203  Laatzen, Laatzen Schatzfund  Imitation   \n",
       "4  3525       169          203  Laatzen, Laatzen Schatzfund   Official   \n",
       "\n",
       "          Denomination             Issuer                   Mint     Date  \\\n",
       "0              Siliqua  Constantinus III.               Lugdunum  407-408   \n",
       "1          Miliarensis    Constantius II.       Constantinopolis  351-355   \n",
       "2  Siliqua (reduziert)    Constantius II.                Arelate  355-363   \n",
       "3               Silber   Constantinus II.  Irreguläre Münzstätte  353-360   \n",
       "4          Miliarensis       Iulianus II.               Lugdunum  360-363   \n",
       "\n",
       "                                          References  \\\n",
       "0                                                NaN   \n",
       "1  Coh., 326 RIC 8, 100Mint mark-/-//C·AWeight4.5...   \n",
       "2  Coh., 343 RIC 8, 261/291Mint mark-/-//SCONWeig...   \n",
       "3  RIC 8, wie (Lug) 216/217 Coh., wie 341Secondar...   \n",
       "4                        RIC 8, 209Mint mark-/-//LVG   \n",
       "\n",
       "                                             Remarks  \\\n",
       "0                       Nach Foto bestimmt.Weight1.6   \n",
       "1  \\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "2  \\n\\n\\n\\nDetailed Result\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...   \n",
       "3  Von Berger als barbarische Nachahmung ausgewie...   \n",
       "4  Datierung nach RIC (abweichend vom FMRD-Band, ...   \n",
       "\n",
       "                                        Bibliography  Date Min  Date Max  \\\n",
       "0  Böhme, Altkönig, Der \"Altkönig\" im Taunus als ...       407       408   \n",
       "1  FMRD VII, Die Fundmünzen der Römischen Zeit in...       351       355   \n",
       "2  FMRD VII, Die Fundmünzen der Römischen Zeit in...       355       363   \n",
       "3  FMRD VII, Die Fundmünzen der Römischen Zeit in...       353       360   \n",
       "4  FMRD VII, Die Fundmünzen der Römischen Zeit in...       360       363   \n",
       "\n",
       "       name     lat   long Fundkategorie  Quantity  \n",
       "0  Altkönig  50.212  8.482    Einzelfund         1  \n",
       "1   Laatzen  52.315  9.797    Schatzfund         1  \n",
       "2   Laatzen  52.315  9.797    Schatzfund         1  \n",
       "3   Laatzen  52.315  9.797    Schatzfund         1  \n",
       "4   Laatzen  52.315  9.797    Schatzfund         1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df53facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a final problem. The original scrape and regex cut the last letter off of some finds\n",
    "# This is the result of having to filter through huge variation in string patterns\n",
    "# I don't think an ideal pattern matching formula exists, so rather than play with that\n",
    "# We just reinput them from afe_fundort\n",
    "fundort = pd.read_csv('afe_fundort.csv')\n",
    "# Drop nans\n",
    "fundort = fundort.dropna(subset=['ID'])\n",
    "fundort = fundort.reset_index(drop=True)\n",
    "# Convert to int, not str. Find IDs are ints\n",
    "fundort['ID'] = fundort['ID'].astype(int)\n",
    "def find_corrector(find_id):\n",
    "    find_name = fundort[fundort['ID']==find_id]['Name'].values[0]\n",
    "    return find_name\n",
    "daip['Location'] = daip['afe_find_id'].map(find_corrector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d8c2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create finds using the same method.\n",
    "# We can fill in information in a subsequent step, by joining between the two\n",
    "#find = daip.groupby(['afe_find_id','Location','Fundkategorie','lat','long','References'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "find = daip.groupby(['afe_find_id','Location','Fundkategorie','lat','long'])[\"Quantity\"].apply(lambda x : x.astype(int).sum())\n",
    "# The process yields a Pandas series. That series should be turned into a df\n",
    "find = find.to_frame()\n",
    "# That df needs to level out the indexes back into normal values.\n",
    "# Each index value simply becomes a repeated value in the relevant cells\n",
    "#find = find.reset_index(level=['afe_find_id','Location','Fundkategorie','lat','long','References'])\n",
    "find = find.reset_index(level=['afe_find_id','Location','Fundkategorie','lat','long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fcbd1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "find['year_found'] = '1700'\n",
    "find['year_found_end'] = today.strftime(\"%Y\")\n",
    "find['comments'] = ''\n",
    "find['imported'] = today.strftime(\"%m-%d-%Y\")\n",
    "find['references'] = ''\n",
    "for row, i in enumerate(find['afe_find_id']):\n",
    "    find['references'].loc[row] = 'http://afe.dainst.org/findspot?afeid=' + str(find['afe_find_id'].loc[row])\n",
    "find['owner'] = 'DAI'\n",
    "find['created'] = today.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92af1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create all of the columns we can.\n",
    "# The only one not possible yet is startDate and endDate, which requires\n",
    "# work to be done on coin groups (return to it below)\n",
    "find['hoard?'] = find['Fundkategorie'] == 'Schatzfund'\n",
    "find['single?'] = find['Fundkategorie'] == 'Einzelfund'\n",
    "find['excavation?'] = find['Fundkategorie'] == 'Kollektivfund'\n",
    "find['excavation?'] = find['Fundkategorie'] == 'Grabfund'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52b93a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "find = find.rename(columns={\"afe_find_id\":\"find_number\",\"Location\":\"Name\",\"Fundkategorie\":\"type_find\",\"Quantity\":\"num_coins\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dfa3674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert find types from German to English. Some additional work needed for categories that don't fit\n",
    "# e.g., kollektivfund\n",
    "find_conversion = {\n",
    "    'Einzelfund':'Single Find',\n",
    "    'Schatzfund':'Hoard Find',\n",
    "    'Excavation':'Excavation Find'\n",
    "}\n",
    "\n",
    "\n",
    "#new_list = find['type_find'].fillna('Unbekannt').apply(lambda x:find_conversion[x])\n",
    "find['type_find'] = new_list\n",
    "\n",
    "for row, i in enumerate(find['type_find']):\n",
    "    if find['num_coins'].loc[row] == 1 and i != ('Single Find'or'Hoard Find'):\n",
    "        find['type_find'].loc[row] = 'Single Find'\n",
    "    elif find['num_coins'].loc[row] > 1 and i != ('Single Find'or'Hoard Find'):\n",
    "        find['type_find'].loc[row] = 'Hoard Find'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c79b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds df is missing certainty values, so we add them here too\n",
    "# This is not something AFE provides insight on, so we assign our highest value\n",
    "find['cf_custom_region_vague'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "39d492d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the goal is to relink the finds to their component detailed coin finds\n",
    "# These have been agglomerated and lost in the above summing process\n",
    "# We just query the daip df by afe_find_id and put it into a new column\n",
    "our_finds = list(find['find_number'])\n",
    "biggest = max(find['num_coins'])\n",
    "all_coins = []\n",
    "# The loop builds a list of lists for each find (all_coins)\n",
    "for i in our_finds:\n",
    "    coins = []\n",
    "    for y in range(biggest):\n",
    "        try:\n",
    "            coin = daip[daip['afe_find_id']==i]['ID_x'].values[y]\n",
    "            coins.append(coin)\n",
    "        except:\n",
    "            continue\n",
    "    all_coins.append(coins)\n",
    "# The new column is made from the all_coins list of lists  \n",
    "find['detailed_find_ids'] = all_coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6b07816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we make a function to take all those coins indexed to finds and turn them into URLs\n",
    "# These are mapped onto a new column, group_refs\n",
    "# That is then concated with references\n",
    "def det_fin_adder(*list_finds):\n",
    "    find_citations = ''\n",
    "    for i in list_finds:\n",
    "        for y in i:\n",
    "            find_citations = find_citations + ' || http://afe.dainst.org/detailedresult?l=en&link=' + str(y)\n",
    "    return find_citations\n",
    "\n",
    "find['group_refs'] = find['detailed_find_ids'].map(det_fin_adder)\n",
    "find['references'] = find['references'] + find['group_refs']\n",
    "\n",
    "# Then we drop the previous columns in favour of just references\n",
    "find = find.drop(columns=['detailed_find_ids','group_refs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43ddd52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>find_number</th>\n",
       "      <th>Name</th>\n",
       "      <th>type_find</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>num_coins</th>\n",
       "      <th>year_found</th>\n",
       "      <th>year_found_end</th>\n",
       "      <th>comments</th>\n",
       "      <th>imported</th>\n",
       "      <th>references</th>\n",
       "      <th>owner</th>\n",
       "      <th>created</th>\n",
       "      <th>hoard?</th>\n",
       "      <th>single?</th>\n",
       "      <th>excavation?</th>\n",
       "      <th>cf_custom_region_vague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Schmalstede II (Brandgrab 136)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.20000</td>\n",
       "      <td>10.03300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=201 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>Laatzen (Hortfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>52.31500</td>\n",
       "      <td>9.79700</td>\n",
       "      <td>4</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=203 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212</td>\n",
       "      <td>Divitz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.32500</td>\n",
       "      <td>12.68300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=212 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214</td>\n",
       "      <td>Demern</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.74200</td>\n",
       "      <td>10.98700</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=214 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>Kittendorf</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.62500</td>\n",
       "      <td>12.90400</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=216 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>218</td>\n",
       "      <td>Radewitz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.29500</td>\n",
       "      <td>14.15500</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=218 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>221</td>\n",
       "      <td>Deersheim I</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.98300</td>\n",
       "      <td>10.78100</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=221 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>222</td>\n",
       "      <td>Altenhausen II</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>52.26200</td>\n",
       "      <td>11.25400</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=222 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>435</td>\n",
       "      <td>Quedlinburg II</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.78800</td>\n",
       "      <td>11.15000</td>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=435 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>745</td>\n",
       "      <td>Berlin-Friedrichshagen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>52.45052</td>\n",
       "      <td>13.62463</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=745 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>786</td>\n",
       "      <td>Kloster</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.58700</td>\n",
       "      <td>13.10600</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=786 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>799</td>\n",
       "      <td>Warnemünde (suspekt)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.17700</td>\n",
       "      <td>12.08400</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=799 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>803</td>\n",
       "      <td>Stralsund IV</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.30242</td>\n",
       "      <td>13.09284</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=803 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>809</td>\n",
       "      <td>Lübtheen II</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>53.31360</td>\n",
       "      <td>11.07660</td>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=809 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>826</td>\n",
       "      <td>\"Mecklenburg\"</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>53.50000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=826 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>842</td>\n",
       "      <td>Schwarzenhof (Forst) Fpl. unbek.</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.46348</td>\n",
       "      <td>12.79838</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=842 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>860</td>\n",
       "      <td>\"Müritzgebiet\" - Fundort unbekannt</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.48222</td>\n",
       "      <td>12.62333</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=860 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>864</td>\n",
       "      <td>Neubrandenburg III</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.56400</td>\n",
       "      <td>13.27500</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=864 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>867</td>\n",
       "      <td>Bautzen II</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.18035</td>\n",
       "      <td>14.43494</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=867 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>876</td>\n",
       "      <td>Holscha (suspekt)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.26829</td>\n",
       "      <td>14.34587</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=876 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>884</td>\n",
       "      <td>Radeberg (suspekt)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.12015</td>\n",
       "      <td>13.91545</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=884 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>899</td>\n",
       "      <td>Nauleis</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.24901</td>\n",
       "      <td>13.58222</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=899 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>915</td>\n",
       "      <td>Sörnewitz (Hortfund?)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.13719</td>\n",
       "      <td>13.52095</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=915 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>927</td>\n",
       "      <td>Zittau I</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.90780</td>\n",
       "      <td>14.80110</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=927 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>928</td>\n",
       "      <td>Zittau II</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.90780</td>\n",
       "      <td>14.80110</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=928 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>939</td>\n",
       "      <td>Striesen (Hortfund?)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.04809</td>\n",
       "      <td>13.78973</td>\n",
       "      <td>8</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=939 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>967</td>\n",
       "      <td>Zwenkau IV</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.21720</td>\n",
       "      <td>12.31440</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=967 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>972</td>\n",
       "      <td>Dehnitz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.35013</td>\n",
       "      <td>12.73436</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=972 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>980</td>\n",
       "      <td>Leipzig-Knauthain</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.27001</td>\n",
       "      <td>12.30183</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=980 || ht...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1038</td>\n",
       "      <td>Niebüll (Schatzfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>54.78700</td>\n",
       "      <td>8.82900</td>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=1038 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1050</td>\n",
       "      <td>Fehmarn II (Schatzfund?)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>54.43300</td>\n",
       "      <td>11.20000</td>\n",
       "      <td>3</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=1050 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1965</td>\n",
       "      <td>Bückeburg (Hortfund) II</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>52.26100</td>\n",
       "      <td>9.04900</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=1965 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2259</td>\n",
       "      <td>Altkönig</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.21200</td>\n",
       "      <td>8.48200</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2259 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2313</td>\n",
       "      <td>Beeckerwerth</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.47600</td>\n",
       "      <td>6.70100</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2313 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2314</td>\n",
       "      <td>Großenbaum (Hortfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.36700</td>\n",
       "      <td>6.78200</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2314 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2399</td>\n",
       "      <td>Bad Sulza</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.08900</td>\n",
       "      <td>11.62500</td>\n",
       "      <td>7</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2399 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2400</td>\n",
       "      <td>Eckolstädt</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.03600</td>\n",
       "      <td>11.64100</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2400 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2403</td>\n",
       "      <td>Reisdorf</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.10200</td>\n",
       "      <td>11.55900</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2403 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2405</td>\n",
       "      <td>Arnstadt (Fundgattung unsicher)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>50.84000</td>\n",
       "      <td>10.95200</td>\n",
       "      <td>4</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2405 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2485</td>\n",
       "      <td>Luckenwalde II (Kollektivfund?)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>52.09000</td>\n",
       "      <td>13.16800</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=2485 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3050</td>\n",
       "      <td>Gorsleben</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.26700</td>\n",
       "      <td>11.18300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3050 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3051</td>\n",
       "      <td>Gorsleben (unsichere Angaben)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.26700</td>\n",
       "      <td>11.18300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3051 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3054</td>\n",
       "      <td>Heldrungen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.30200</td>\n",
       "      <td>11.21800</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3054 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3055</td>\n",
       "      <td>Oberheldrungen</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.28300</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3055 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3066</td>\n",
       "      <td>Vieselbach</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.99900</td>\n",
       "      <td>11.14500</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3066 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3074</td>\n",
       "      <td>Umgebung von Gotha</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.94800</td>\n",
       "      <td>10.70200</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3074 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3080</td>\n",
       "      <td>Mühlberg (Fp. 14)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.86800</td>\n",
       "      <td>10.82300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3080 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3085</td>\n",
       "      <td>Seebergen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.92100</td>\n",
       "      <td>10.79900</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3085 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3086</td>\n",
       "      <td>Sonneborn</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.00000</td>\n",
       "      <td>10.58300</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3086 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3090</td>\n",
       "      <td>Wandersleben</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.89900</td>\n",
       "      <td>10.85000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3090 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3103</td>\n",
       "      <td>Neunheilingen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.20000</td>\n",
       "      <td>10.66700</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3103 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3109</td>\n",
       "      <td>Grossengottern</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.15000</td>\n",
       "      <td>10.58300</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3109 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3111</td>\n",
       "      <td>Horsmar</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.28000</td>\n",
       "      <td>10.40700</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3111 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3118</td>\n",
       "      <td>Mühlhausen/Thüringen (Grabfunde)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.20900</td>\n",
       "      <td>10.45300</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3118 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3123</td>\n",
       "      <td>Bei Schlotheim</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.24700</td>\n",
       "      <td>10.65600</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3123 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3130</td>\n",
       "      <td>Umgebung von Beichlingen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.23300</td>\n",
       "      <td>11.25000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3130 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3142</td>\n",
       "      <td>Sömmerda</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.15900</td>\n",
       "      <td>11.11500</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3142 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3154</td>\n",
       "      <td>Kleinschwabhausen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.93200</td>\n",
       "      <td>11.46100</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3154 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3158</td>\n",
       "      <td>Grossbodungen (Hortfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.47600</td>\n",
       "      <td>10.48100</td>\n",
       "      <td>21</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3158 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3163</td>\n",
       "      <td>Erfurt (Innenstadt) (Grabfund)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.97900</td>\n",
       "      <td>11.03300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3163 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3165</td>\n",
       "      <td>Erfurt-Bischleben (Hortfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>50.93600</td>\n",
       "      <td>10.98500</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3165 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3166</td>\n",
       "      <td>Erfurt-Bischleben (Grabfunde)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>50.93600</td>\n",
       "      <td>10.98500</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3166 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3169</td>\n",
       "      <td>Weimar (Grabfunde)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>50.98000</td>\n",
       "      <td>11.32900</td>\n",
       "      <td>4</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3169 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3170</td>\n",
       "      <td>Meuselwitz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.04300</td>\n",
       "      <td>12.29900</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3170 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3176</td>\n",
       "      <td>Lützeroda</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.96000</td>\n",
       "      <td>11.55300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3176 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3177</td>\n",
       "      <td>Maua</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.86900</td>\n",
       "      <td>11.60000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3177 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3180</td>\n",
       "      <td>Köstitz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.70000</td>\n",
       "      <td>11.28300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3180 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3190</td>\n",
       "      <td>Jena</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.92900</td>\n",
       "      <td>11.59000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3190 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3211</td>\n",
       "      <td>Milz</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.37800</td>\n",
       "      <td>10.53800</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3211 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3219</td>\n",
       "      <td>Kühndorf</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>50.61700</td>\n",
       "      <td>10.48300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3219 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3291</td>\n",
       "      <td>Raguth Fpl. 5 (Hortfund)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.56866</td>\n",
       "      <td>11.05225</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3291 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3301</td>\n",
       "      <td>Babke 3</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>53.36110</td>\n",
       "      <td>12.88868</td>\n",
       "      <td>4</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3301 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3376</td>\n",
       "      <td>Wendisch-Paulsdorf</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.08320</td>\n",
       "      <td>14.65900</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3376 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3380</td>\n",
       "      <td>Lercha</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>51.16490</td>\n",
       "      <td>13.48590</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3380 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3381</td>\n",
       "      <td>Lercha (Siedlung)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.16490</td>\n",
       "      <td>13.48590</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3381 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3390</td>\n",
       "      <td>Kitzen</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.22172</td>\n",
       "      <td>12.22349</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3390 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3690</td>\n",
       "      <td>Toitin Fpl. 28</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.91670</td>\n",
       "      <td>13.35000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3690 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3720</td>\n",
       "      <td>Schmalstede I (Brandgrab 134)</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>54.20000</td>\n",
       "      <td>10.03300</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3720 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>3725</td>\n",
       "      <td>Gützkow, Fpl. 76 (Hortfund)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>53.95000</td>\n",
       "      <td>13.41670</td>\n",
       "      <td>5</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3725 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3778</td>\n",
       "      <td>Quedlinburg IV</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.78800</td>\n",
       "      <td>11.15000</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3778 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3807</td>\n",
       "      <td>Gädebehn Fpl. 10</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>53.63330</td>\n",
       "      <td>13.06670</td>\n",
       "      <td>2</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3807 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3833</td>\n",
       "      <td>bei Wachtendonk</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>51.39740</td>\n",
       "      <td>6.29190</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3833 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3878</td>\n",
       "      <td>Plau Fpl. 125</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.45000</td>\n",
       "      <td>12.26670</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3878 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3996</td>\n",
       "      <td>Fredenbeck</td>\n",
       "      <td>Single Find</td>\n",
       "      <td>53.52360</td>\n",
       "      <td>9.40350</td>\n",
       "      <td>1</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=3996 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4004</td>\n",
       "      <td>Fehmarn III (Schatzfund?)</td>\n",
       "      <td>Hoard Find</td>\n",
       "      <td>54.43300</td>\n",
       "      <td>11.20000</td>\n",
       "      <td>4</td>\n",
       "      <td>1700</td>\n",
       "      <td>2023</td>\n",
       "      <td></td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>http://afe.dainst.org/findspot?afeid=4004 || h...</td>\n",
       "      <td>DAI</td>\n",
       "      <td>06-15-2023</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    find_number                                Name    type_find       lat  \\\n",
       "0           201      Schmalstede II (Brandgrab 136)  Single Find  54.20000   \n",
       "1           203                  Laatzen (Hortfund)   Hoard Find  52.31500   \n",
       "2           212                              Divitz  Single Find  54.32500   \n",
       "3           214                              Demern  Single Find  53.74200   \n",
       "4           216                          Kittendorf  Single Find  53.62500   \n",
       "5           218                            Radewitz  Single Find  53.29500   \n",
       "6           221                         Deersheim I  Single Find  51.98300   \n",
       "7           222                      Altenhausen II  Single Find  52.26200   \n",
       "8           435                      Quedlinburg II   Hoard Find  51.78800   \n",
       "9           745              Berlin-Friedrichshagen  Single Find  52.45052   \n",
       "10          786                             Kloster  Single Find  54.58700   \n",
       "11          799                Warnemünde (suspekt)  Single Find  54.17700   \n",
       "12          803                        Stralsund IV  Single Find  54.30242   \n",
       "13          809                         Lübtheen II   Hoard Find  53.31360   \n",
       "14          826                       \"Mecklenburg\"   Hoard Find  53.50000   \n",
       "15          842    Schwarzenhof (Forst) Fpl. unbek.  Single Find  53.46348   \n",
       "16          860  \"Müritzgebiet\" - Fundort unbekannt  Single Find  53.48222   \n",
       "17          864                  Neubrandenburg III  Single Find  53.56400   \n",
       "18          867                          Bautzen II  Single Find  51.18035   \n",
       "19          876                   Holscha (suspekt)  Single Find  51.26829   \n",
       "20          884                  Radeberg (suspekt)   Hoard Find  51.12015   \n",
       "21          899                             Nauleis  Single Find  51.24901   \n",
       "22          915               Sörnewitz (Hortfund?)  Single Find  51.13719   \n",
       "23          927                            Zittau I  Single Find  50.90780   \n",
       "24          928                           Zittau II  Single Find  50.90780   \n",
       "25          939                Striesen (Hortfund?)   Hoard Find  51.04809   \n",
       "26          967                          Zwenkau IV  Single Find  51.21720   \n",
       "27          972                             Dehnitz  Single Find  51.35013   \n",
       "28          980                   Leipzig-Knauthain  Single Find  51.27001   \n",
       "29         1038                Niebüll (Schatzfund)   Hoard Find  54.78700   \n",
       "30         1050            Fehmarn II (Schatzfund?)   Hoard Find  54.43300   \n",
       "31         1965             Bückeburg (Hortfund) II   Hoard Find  52.26100   \n",
       "32         2259                            Altkönig  Single Find  50.21200   \n",
       "33         2313                        Beeckerwerth  Single Find  51.47600   \n",
       "34         2314               Großenbaum (Hortfund)   Hoard Find  51.36700   \n",
       "35         2399                           Bad Sulza   Hoard Find  51.08900   \n",
       "36         2400                          Eckolstädt  Single Find  51.03600   \n",
       "37         2403                            Reisdorf  Single Find  51.10200   \n",
       "38         2405     Arnstadt (Fundgattung unsicher)   Hoard Find  50.84000   \n",
       "39         2485     Luckenwalde II (Kollektivfund?)  Single Find  52.09000   \n",
       "40         3050                           Gorsleben  Single Find  51.26700   \n",
       "41         3051       Gorsleben (unsichere Angaben)  Single Find  51.26700   \n",
       "42         3054                          Heldrungen  Single Find  51.30200   \n",
       "43         3055                      Oberheldrungen   Hoard Find  51.28300   \n",
       "44         3066                          Vieselbach  Single Find  50.99900   \n",
       "45         3074                  Umgebung von Gotha  Single Find  50.94800   \n",
       "46         3080                   Mühlberg (Fp. 14)  Single Find  50.86800   \n",
       "47         3085                           Seebergen  Single Find  50.92100   \n",
       "48         3086                           Sonneborn   Hoard Find  51.00000   \n",
       "49         3090                        Wandersleben  Single Find  50.89900   \n",
       "50         3103                       Neunheilingen  Single Find  51.20000   \n",
       "51         3109                      Grossengottern   Hoard Find  51.15000   \n",
       "52         3111                             Horsmar  Single Find  51.28000   \n",
       "53         3118    Mühlhausen/Thüringen (Grabfunde)   Hoard Find  51.20900   \n",
       "54         3123                      Bei Schlotheim  Single Find  51.24700   \n",
       "55         3130            Umgebung von Beichlingen  Single Find  51.23300   \n",
       "56         3142                            Sömmerda  Single Find  51.15900   \n",
       "57         3154                   Kleinschwabhausen  Single Find  50.93200   \n",
       "58         3158            Grossbodungen (Hortfund)   Hoard Find  51.47600   \n",
       "59         3163      Erfurt (Innenstadt) (Grabfund)  Single Find  50.97900   \n",
       "60         3165        Erfurt-Bischleben (Hortfund)   Hoard Find  50.93600   \n",
       "61         3166       Erfurt-Bischleben (Grabfunde)   Hoard Find  50.93600   \n",
       "62         3169                  Weimar (Grabfunde)   Hoard Find  50.98000   \n",
       "63         3170                          Meuselwitz  Single Find  51.04300   \n",
       "64         3176                           Lützeroda  Single Find  50.96000   \n",
       "65         3177                                Maua  Single Find  50.86900   \n",
       "66         3180                             Köstitz  Single Find  50.70000   \n",
       "67         3190                                Jena  Single Find  50.92900   \n",
       "68         3211                                Milz  Single Find  50.37800   \n",
       "69         3219                            Kühndorf  Single Find  50.61700   \n",
       "70         3291            Raguth Fpl. 5 (Hortfund)  Single Find  53.56866   \n",
       "71         3301                             Babke 3   Hoard Find  53.36110   \n",
       "72         3376                  Wendisch-Paulsdorf  Single Find  51.08320   \n",
       "73         3380                              Lercha   Hoard Find  51.16490   \n",
       "74         3381                   Lercha (Siedlung)  Single Find  51.16490   \n",
       "75         3390                              Kitzen  Single Find  51.22172   \n",
       "76         3690                      Toitin Fpl. 28  Single Find  53.91670   \n",
       "77         3720       Schmalstede I (Brandgrab 134)  Single Find  54.20000   \n",
       "78         3725         Gützkow, Fpl. 76 (Hortfund)   Hoard Find  53.95000   \n",
       "79         3778                      Quedlinburg IV  Single Find  51.78800   \n",
       "80         3807                    Gädebehn Fpl. 10   Hoard Find  53.63330   \n",
       "81         3833                     bei Wachtendonk  Single Find  51.39740   \n",
       "82         3878                       Plau Fpl. 125  Single Find  53.45000   \n",
       "83         3996                          Fredenbeck  Single Find  53.52360   \n",
       "84         4004           Fehmarn III (Schatzfund?)   Hoard Find  54.43300   \n",
       "\n",
       "        long  num_coins year_found year_found_end comments    imported  \\\n",
       "0   10.03300          1       1700           2023           06-15-2023   \n",
       "1    9.79700          4       1700           2023           06-15-2023   \n",
       "2   12.68300          1       1700           2023           06-15-2023   \n",
       "3   10.98700          1       1700           2023           06-15-2023   \n",
       "4   12.90400          1       1700           2023           06-15-2023   \n",
       "5   14.15500          1       1700           2023           06-15-2023   \n",
       "6   10.78100          1       1700           2023           06-15-2023   \n",
       "7   11.25400          1       1700           2023           06-15-2023   \n",
       "8   11.15000          3       1700           2023           06-15-2023   \n",
       "9   13.62463          1       1700           2023           06-15-2023   \n",
       "10  13.10600          1       1700           2023           06-15-2023   \n",
       "11  12.08400          1       1700           2023           06-15-2023   \n",
       "12  13.09284          1       1700           2023           06-15-2023   \n",
       "13  11.07660          3       1700           2023           06-15-2023   \n",
       "14  12.00000          2       1700           2023           06-15-2023   \n",
       "15  12.79838          1       1700           2023           06-15-2023   \n",
       "16  12.62333          1       1700           2023           06-15-2023   \n",
       "17  13.27500          1       1700           2023           06-15-2023   \n",
       "18  14.43494          1       1700           2023           06-15-2023   \n",
       "19  14.34587          1       1700           2023           06-15-2023   \n",
       "20  13.91545          2       1700           2023           06-15-2023   \n",
       "21  13.58222          1       1700           2023           06-15-2023   \n",
       "22  13.52095          1       1700           2023           06-15-2023   \n",
       "23  14.80110          1       1700           2023           06-15-2023   \n",
       "24  14.80110          1       1700           2023           06-15-2023   \n",
       "25  13.78973          8       1700           2023           06-15-2023   \n",
       "26  12.31440          1       1700           2023           06-15-2023   \n",
       "27  12.73436          1       1700           2023           06-15-2023   \n",
       "28  12.30183          1       1700           2023           06-15-2023   \n",
       "29   8.82900          3       1700           2023           06-15-2023   \n",
       "30  11.20000          3       1700           2023           06-15-2023   \n",
       "31   9.04900          2       1700           2023           06-15-2023   \n",
       "32   8.48200          1       1700           2023           06-15-2023   \n",
       "33   6.70100          1       1700           2023           06-15-2023   \n",
       "34   6.78200          2       1700           2023           06-15-2023   \n",
       "35  11.62500          7       1700           2023           06-15-2023   \n",
       "36  11.64100          1       1700           2023           06-15-2023   \n",
       "37  11.55900          1       1700           2023           06-15-2023   \n",
       "38  10.95200          4       1700           2023           06-15-2023   \n",
       "39  13.16800          1       1700           2023           06-15-2023   \n",
       "40  11.18300          1       1700           2023           06-15-2023   \n",
       "41  11.18300          1       1700           2023           06-15-2023   \n",
       "42  11.21800          1       1700           2023           06-15-2023   \n",
       "43  11.25000          2       1700           2023           06-15-2023   \n",
       "44  11.14500          1       1700           2023           06-15-2023   \n",
       "45  10.70200          1       1700           2023           06-15-2023   \n",
       "46  10.82300          1       1700           2023           06-15-2023   \n",
       "47  10.79900          1       1700           2023           06-15-2023   \n",
       "48  10.58300          2       1700           2023           06-15-2023   \n",
       "49  10.85000          1       1700           2023           06-15-2023   \n",
       "50  10.66700          1       1700           2023           06-15-2023   \n",
       "51  10.58300          2       1700           2023           06-15-2023   \n",
       "52  10.40700          1       1700           2023           06-15-2023   \n",
       "53  10.45300          2       1700           2023           06-15-2023   \n",
       "54  10.65600          1       1700           2023           06-15-2023   \n",
       "55  11.25000          1       1700           2023           06-15-2023   \n",
       "56  11.11500          1       1700           2023           06-15-2023   \n",
       "57  11.46100          1       1700           2023           06-15-2023   \n",
       "58  10.48100         21       1700           2023           06-15-2023   \n",
       "59  11.03300          1       1700           2023           06-15-2023   \n",
       "60  10.98500          2       1700           2023           06-15-2023   \n",
       "61  10.98500          2       1700           2023           06-15-2023   \n",
       "62  11.32900          4       1700           2023           06-15-2023   \n",
       "63  12.29900          1       1700           2023           06-15-2023   \n",
       "64  11.55300          1       1700           2023           06-15-2023   \n",
       "65  11.60000          1       1700           2023           06-15-2023   \n",
       "66  11.28300          1       1700           2023           06-15-2023   \n",
       "67  11.59000          1       1700           2023           06-15-2023   \n",
       "68  10.53800          1       1700           2023           06-15-2023   \n",
       "69  10.48300          1       1700           2023           06-15-2023   \n",
       "70  11.05225          1       1700           2023           06-15-2023   \n",
       "71  12.88868          4       1700           2023           06-15-2023   \n",
       "72  14.65900          1       1700           2023           06-15-2023   \n",
       "73  13.48590          2       1700           2023           06-15-2023   \n",
       "74  13.48590          1       1700           2023           06-15-2023   \n",
       "75  12.22349          1       1700           2023           06-15-2023   \n",
       "76  13.35000          1       1700           2023           06-15-2023   \n",
       "77  10.03300          1       1700           2023           06-15-2023   \n",
       "78  13.41670          5       1700           2023           06-15-2023   \n",
       "79  11.15000          1       1700           2023           06-15-2023   \n",
       "80  13.06670          2       1700           2023           06-15-2023   \n",
       "81   6.29190          1       1700           2023           06-15-2023   \n",
       "82  12.26670          1       1700           2023           06-15-2023   \n",
       "83   9.40350          1       1700           2023           06-15-2023   \n",
       "84  11.20000          4       1700           2023           06-15-2023   \n",
       "\n",
       "                                           references owner     created  \\\n",
       "0   http://afe.dainst.org/findspot?afeid=201 || ht...   DAI  06-15-2023   \n",
       "1   http://afe.dainst.org/findspot?afeid=203 || ht...   DAI  06-15-2023   \n",
       "2   http://afe.dainst.org/findspot?afeid=212 || ht...   DAI  06-15-2023   \n",
       "3   http://afe.dainst.org/findspot?afeid=214 || ht...   DAI  06-15-2023   \n",
       "4   http://afe.dainst.org/findspot?afeid=216 || ht...   DAI  06-15-2023   \n",
       "5   http://afe.dainst.org/findspot?afeid=218 || ht...   DAI  06-15-2023   \n",
       "6   http://afe.dainst.org/findspot?afeid=221 || ht...   DAI  06-15-2023   \n",
       "7   http://afe.dainst.org/findspot?afeid=222 || ht...   DAI  06-15-2023   \n",
       "8   http://afe.dainst.org/findspot?afeid=435 || ht...   DAI  06-15-2023   \n",
       "9   http://afe.dainst.org/findspot?afeid=745 || ht...   DAI  06-15-2023   \n",
       "10  http://afe.dainst.org/findspot?afeid=786 || ht...   DAI  06-15-2023   \n",
       "11  http://afe.dainst.org/findspot?afeid=799 || ht...   DAI  06-15-2023   \n",
       "12  http://afe.dainst.org/findspot?afeid=803 || ht...   DAI  06-15-2023   \n",
       "13  http://afe.dainst.org/findspot?afeid=809 || ht...   DAI  06-15-2023   \n",
       "14  http://afe.dainst.org/findspot?afeid=826 || ht...   DAI  06-15-2023   \n",
       "15  http://afe.dainst.org/findspot?afeid=842 || ht...   DAI  06-15-2023   \n",
       "16  http://afe.dainst.org/findspot?afeid=860 || ht...   DAI  06-15-2023   \n",
       "17  http://afe.dainst.org/findspot?afeid=864 || ht...   DAI  06-15-2023   \n",
       "18  http://afe.dainst.org/findspot?afeid=867 || ht...   DAI  06-15-2023   \n",
       "19  http://afe.dainst.org/findspot?afeid=876 || ht...   DAI  06-15-2023   \n",
       "20  http://afe.dainst.org/findspot?afeid=884 || ht...   DAI  06-15-2023   \n",
       "21  http://afe.dainst.org/findspot?afeid=899 || ht...   DAI  06-15-2023   \n",
       "22  http://afe.dainst.org/findspot?afeid=915 || ht...   DAI  06-15-2023   \n",
       "23  http://afe.dainst.org/findspot?afeid=927 || ht...   DAI  06-15-2023   \n",
       "24  http://afe.dainst.org/findspot?afeid=928 || ht...   DAI  06-15-2023   \n",
       "25  http://afe.dainst.org/findspot?afeid=939 || ht...   DAI  06-15-2023   \n",
       "26  http://afe.dainst.org/findspot?afeid=967 || ht...   DAI  06-15-2023   \n",
       "27  http://afe.dainst.org/findspot?afeid=972 || ht...   DAI  06-15-2023   \n",
       "28  http://afe.dainst.org/findspot?afeid=980 || ht...   DAI  06-15-2023   \n",
       "29  http://afe.dainst.org/findspot?afeid=1038 || h...   DAI  06-15-2023   \n",
       "30  http://afe.dainst.org/findspot?afeid=1050 || h...   DAI  06-15-2023   \n",
       "31  http://afe.dainst.org/findspot?afeid=1965 || h...   DAI  06-15-2023   \n",
       "32  http://afe.dainst.org/findspot?afeid=2259 || h...   DAI  06-15-2023   \n",
       "33  http://afe.dainst.org/findspot?afeid=2313 || h...   DAI  06-15-2023   \n",
       "34  http://afe.dainst.org/findspot?afeid=2314 || h...   DAI  06-15-2023   \n",
       "35  http://afe.dainst.org/findspot?afeid=2399 || h...   DAI  06-15-2023   \n",
       "36  http://afe.dainst.org/findspot?afeid=2400 || h...   DAI  06-15-2023   \n",
       "37  http://afe.dainst.org/findspot?afeid=2403 || h...   DAI  06-15-2023   \n",
       "38  http://afe.dainst.org/findspot?afeid=2405 || h...   DAI  06-15-2023   \n",
       "39  http://afe.dainst.org/findspot?afeid=2485 || h...   DAI  06-15-2023   \n",
       "40  http://afe.dainst.org/findspot?afeid=3050 || h...   DAI  06-15-2023   \n",
       "41  http://afe.dainst.org/findspot?afeid=3051 || h...   DAI  06-15-2023   \n",
       "42  http://afe.dainst.org/findspot?afeid=3054 || h...   DAI  06-15-2023   \n",
       "43  http://afe.dainst.org/findspot?afeid=3055 || h...   DAI  06-15-2023   \n",
       "44  http://afe.dainst.org/findspot?afeid=3066 || h...   DAI  06-15-2023   \n",
       "45  http://afe.dainst.org/findspot?afeid=3074 || h...   DAI  06-15-2023   \n",
       "46  http://afe.dainst.org/findspot?afeid=3080 || h...   DAI  06-15-2023   \n",
       "47  http://afe.dainst.org/findspot?afeid=3085 || h...   DAI  06-15-2023   \n",
       "48  http://afe.dainst.org/findspot?afeid=3086 || h...   DAI  06-15-2023   \n",
       "49  http://afe.dainst.org/findspot?afeid=3090 || h...   DAI  06-15-2023   \n",
       "50  http://afe.dainst.org/findspot?afeid=3103 || h...   DAI  06-15-2023   \n",
       "51  http://afe.dainst.org/findspot?afeid=3109 || h...   DAI  06-15-2023   \n",
       "52  http://afe.dainst.org/findspot?afeid=3111 || h...   DAI  06-15-2023   \n",
       "53  http://afe.dainst.org/findspot?afeid=3118 || h...   DAI  06-15-2023   \n",
       "54  http://afe.dainst.org/findspot?afeid=3123 || h...   DAI  06-15-2023   \n",
       "55  http://afe.dainst.org/findspot?afeid=3130 || h...   DAI  06-15-2023   \n",
       "56  http://afe.dainst.org/findspot?afeid=3142 || h...   DAI  06-15-2023   \n",
       "57  http://afe.dainst.org/findspot?afeid=3154 || h...   DAI  06-15-2023   \n",
       "58  http://afe.dainst.org/findspot?afeid=3158 || h...   DAI  06-15-2023   \n",
       "59  http://afe.dainst.org/findspot?afeid=3163 || h...   DAI  06-15-2023   \n",
       "60  http://afe.dainst.org/findspot?afeid=3165 || h...   DAI  06-15-2023   \n",
       "61  http://afe.dainst.org/findspot?afeid=3166 || h...   DAI  06-15-2023   \n",
       "62  http://afe.dainst.org/findspot?afeid=3169 || h...   DAI  06-15-2023   \n",
       "63  http://afe.dainst.org/findspot?afeid=3170 || h...   DAI  06-15-2023   \n",
       "64  http://afe.dainst.org/findspot?afeid=3176 || h...   DAI  06-15-2023   \n",
       "65  http://afe.dainst.org/findspot?afeid=3177 || h...   DAI  06-15-2023   \n",
       "66  http://afe.dainst.org/findspot?afeid=3180 || h...   DAI  06-15-2023   \n",
       "67  http://afe.dainst.org/findspot?afeid=3190 || h...   DAI  06-15-2023   \n",
       "68  http://afe.dainst.org/findspot?afeid=3211 || h...   DAI  06-15-2023   \n",
       "69  http://afe.dainst.org/findspot?afeid=3219 || h...   DAI  06-15-2023   \n",
       "70  http://afe.dainst.org/findspot?afeid=3291 || h...   DAI  06-15-2023   \n",
       "71  http://afe.dainst.org/findspot?afeid=3301 || h...   DAI  06-15-2023   \n",
       "72  http://afe.dainst.org/findspot?afeid=3376 || h...   DAI  06-15-2023   \n",
       "73  http://afe.dainst.org/findspot?afeid=3380 || h...   DAI  06-15-2023   \n",
       "74  http://afe.dainst.org/findspot?afeid=3381 || h...   DAI  06-15-2023   \n",
       "75  http://afe.dainst.org/findspot?afeid=3390 || h...   DAI  06-15-2023   \n",
       "76  http://afe.dainst.org/findspot?afeid=3690 || h...   DAI  06-15-2023   \n",
       "77  http://afe.dainst.org/findspot?afeid=3720 || h...   DAI  06-15-2023   \n",
       "78  http://afe.dainst.org/findspot?afeid=3725 || h...   DAI  06-15-2023   \n",
       "79  http://afe.dainst.org/findspot?afeid=3778 || h...   DAI  06-15-2023   \n",
       "80  http://afe.dainst.org/findspot?afeid=3807 || h...   DAI  06-15-2023   \n",
       "81  http://afe.dainst.org/findspot?afeid=3833 || h...   DAI  06-15-2023   \n",
       "82  http://afe.dainst.org/findspot?afeid=3878 || h...   DAI  06-15-2023   \n",
       "83  http://afe.dainst.org/findspot?afeid=3996 || h...   DAI  06-15-2023   \n",
       "84  http://afe.dainst.org/findspot?afeid=4004 || h...   DAI  06-15-2023   \n",
       "\n",
       "    hoard?  single?  excavation? cf_custom_region_vague  \n",
       "0    False    False         True                      2  \n",
       "1     True    False        False                      2  \n",
       "2    False     True        False                      2  \n",
       "3    False     True        False                      2  \n",
       "4    False    False         True                      2  \n",
       "5    False     True        False                      2  \n",
       "6    False    False         True                      2  \n",
       "7    False     True        False                      2  \n",
       "8    False     True        False                      2  \n",
       "9    False     True        False                      2  \n",
       "10   False     True        False                      2  \n",
       "11   False     True        False                      2  \n",
       "12   False     True        False                      2  \n",
       "13   False    False        False                      2  \n",
       "14   False     True        False                      2  \n",
       "15   False     True        False                      2  \n",
       "16   False     True        False                      2  \n",
       "17   False     True        False                      2  \n",
       "18   False     True        False                      2  \n",
       "19   False    False        False                      2  \n",
       "20   False    False        False                      2  \n",
       "21   False     True        False                      2  \n",
       "22    True    False        False                      2  \n",
       "23   False     True        False                      2  \n",
       "24   False    False        False                      2  \n",
       "25    True    False        False                      2  \n",
       "26   False     True        False                      2  \n",
       "27   False     True        False                      2  \n",
       "28   False     True        False                      2  \n",
       "29    True    False        False                      2  \n",
       "30   False    False        False                      2  \n",
       "31    True    False        False                      2  \n",
       "32   False     True        False                      2  \n",
       "33   False     True        False                      2  \n",
       "34    True    False        False                      2  \n",
       "35   False    False        False                      2  \n",
       "36   False     True        False                      2  \n",
       "37   False    False        False                      2  \n",
       "38   False    False        False                      2  \n",
       "39   False    False        False                      2  \n",
       "40   False     True        False                      2  \n",
       "41   False     True        False                      2  \n",
       "42   False    False         True                      2  \n",
       "43   False    False        False                      2  \n",
       "44   False    False        False                      2  \n",
       "45   False     True        False                      2  \n",
       "46   False    False        False                      2  \n",
       "47   False    False        False                      2  \n",
       "48   False    False        False                      2  \n",
       "49   False    False        False                      2  \n",
       "50   False    False        False                      2  \n",
       "51   False    False        False                      2  \n",
       "52   False    False        False                      2  \n",
       "53   False    False         True                      2  \n",
       "54   False    False        False                      2  \n",
       "55   False     True        False                      2  \n",
       "56   False    False        False                      2  \n",
       "57   False    False        False                      2  \n",
       "58    True    False        False                      2  \n",
       "59   False    False         True                      2  \n",
       "60    True    False        False                      2  \n",
       "61   False    False         True                      2  \n",
       "62   False    False         True                      2  \n",
       "63   False    False         True                      2  \n",
       "64   False     True        False                      2  \n",
       "65   False    False        False                      2  \n",
       "66   False    False         True                      2  \n",
       "67   False    False        False                      2  \n",
       "68   False     True        False                      2  \n",
       "69   False    False        False                      2  \n",
       "70    True    False        False                      2  \n",
       "71   False     True        False                      2  \n",
       "72   False    False        False                      2  \n",
       "73   False    False        False                      2  \n",
       "74   False    False        False                      2  \n",
       "75   False     True        False                      2  \n",
       "76   False     True        False                      2  \n",
       "77   False    False         True                      2  \n",
       "78    True    False        False                      2  \n",
       "79   False     True        False                      2  \n",
       "80   False     True        False                      2  \n",
       "81   False     True        False                      2  \n",
       "82   False     True        False                      2  \n",
       "83   False     True        False                      2  \n",
       "84    True    False        False                      2  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "944d0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "find.to_csv(path_or_buf='final_afe_finds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a4886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
